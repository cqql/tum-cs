\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% Define the page margin
\usepackage[margin=3cm]{geometry}

% Better typography (font rendering)
\usepackage{microtype}

% Math environments and macros
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

% Define \includegraphics to include graphics
\usepackage{graphicx}

% Syntax highlighting
\usepackage{minted}

% Set global minted options
\setminted{linenos, autogobble, mathescape, numbersep=5pt}

% Import the comment environment for orgtbl-mode
\usepackage{comment}

% Do not indent paragraphs
\usepackage{parskip}

\newtheoremstyle{thmstyle}
  {6pt} % Space above
  {6pt} % Space below
  {} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec (can be left empty, meaning `normal')
\theoremstyle{thmstyle}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}
\newtheorem{algorithm}{Algorithm}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\argmin}{arg\,min}

\newcommand{\diff}{~\mathrm{d}}
\newcommand{\E}{\mathbb{E}~}
\renewcommand{\Pr}[1]{\mathbb{P}\left( #1 \right)}

\title{Compressed Compressed Sensing Script}
\author{Marten Lienen}
\date{}

\begin{document}

\maketitle

This will list only definitions, lemmata and theorems but no proofs.

\section{Introduction}

\subsection{Imaging}

\subsection{Machine Learning}

\subsection{Sparsity and Compressibility}

\begin{notation}
  \begin{description}
  \item[$p$-Norm]
    \begin{equation*}
      ||x||_{p} = \left( \sum_{j = 1}^{N} |x_{j}|^{p} \right)^{\frac{1}{p}}, \quad 0 < p < \infty
    \end{equation*}
  \item[$\infty$-quasinorm]
    \begin{equation*}
      ||x||_{\infty} = \sup_{j} |x_{j}|
    \end{equation*}
  \item[Sequence from $1$ to $N$]
    \begin{equation*}
      [N] = \{ 1, \dots, N \}
    \end{equation*}
  \end{description}
\end{notation}

\begin{definition}[sparsity, support]
  For a vector $x \in \mathbb{R}^{N}$ or $\mathbb{C}^{N}$ its \emph{support} is defined as
  \begin{equation*}
    \supp x = \{ k : x_{k} \ne 0 \}
  \end{equation*}
  $x$ is \emph{$k$-sparse} if $\supp x \le k$.
  Furthermore, denote $||x||_{0} := |\supp x|$; with some abuse of notation we speak of the $0$-norm (as $||x||_{p} \rightarrow_{p \rightarrow 0} ||x||0$).
\end{definition}

\begin{definition}[error of best $s$-term approximation]
  The error of the best $s$-term approximation of a vector $x \in \mathbb{R}^{N}$ or $\mathbb{C}^{N}$ is
  \begin{equation*}
    \sigma_{s}(x)_{p} := \inf \{ ||x - z||_{p} : z~\text{is $s$-sparse} \}
  \end{equation*}
\end{definition}

\begin{definition}
  A vector $x$ is called \emph{compressible} if $\sigma_{s}(x)_{p}$ decays quickly in $s$.
\end{definition}

\begin{proposition}[Stechkin]
  For any $q > p > 0$ and any $x \in \mathbb{C}^{N}$,
  \begin{equation*}
    \sigma_{s}(x)_{q} \le \frac{1}{s^{\frac{1}{p} - \frac{1}{q}}} ||x||_{p}
  \end{equation*}
\end{proposition}

\begin{definition}
  The \emph{non-increasing rearrangement} of $x \in \mathbb{C}^{N}$ is the vector $x^{*} \in \mathbb{R}_{+}^{N}$ for which
  \begin{equation*}
    x_{1}^{*} \ge x_{2}^{*} \ge \dots \ge x_{N}^{*} \ge 0
  \end{equation*}
  and there is a permutation $\pi : [N] \rightarrow [N]$ with $x_{j}^{*} = |x_{\pi(j)}|$ for all $j \in [N]$.
\end{definition}

\begin{remark}
  Using convex analysis the constant can be refined to obtain
  \begin{equation*}
    \sigma_{s}(x)_{q} \le \frac{c_{p,q}}{s^{\frac{1}{p} - \frac{1}{q}}} ||x||_{p}
  \end{equation*}
  where
  \begin{equation*}
    c_{p,q} = \left[ \left( \frac{p}{q} \right)^{\frac{p}{q}} \left( 1 - \frac{p}{1} \right)^{1 - \frac{p}{q}} \right]^{\frac{1}{p}}
  \end{equation*}
  For $p = 1, q = 2$ this yields $\sigma_{s}(x)_{2} \le \frac{1}{2\sqrt{s}} ||x||_{1}$.
\end{remark}

\subsection{The Compressed Sensing Problem}

Solve $Ax = y$ for $x$ knowing that $x$ is sparse (or compressible).
Does this problem even have a unique solution?
Not always, for example choose $A$ as a subset of rows of $I$.

\begin{theorem}
  Given $A \in \mathbb{C}^{m \times N}$, the following properties are equivalent.
  \begin{itemize}
  \item every $s$-sparse vector $x \in \mathbb{C}^{N}$ is the unique $s$-sparse solution of $Az = Ax$, that is, if $Ax = Az$ and both $x$ ans $z$ are $s$-sparse, then $x = z$.
  \item The null space $\Ker A$ does not contain any $2s$-sparse vector other than the zero vector, that is, $(\Ker A) \cap \{ z \in \mathbb{C}^{N} : ||z||_{0} \le 2s \} = \{ 0 \}$.
  \item For every $S \subset [N]$ with $\card(s) \le 2s$, the submatrix $A_{S}$ is injective.
  \item Every set of $2s$ columns of $A$ is linearly independent.
  \end{itemize}
\end{theorem}

\begin{theorem}
  For any $N \ge 2s$, there exists a measurement matrix $A \in \mathbb{C}^{m \times N}$ with $m = 2s$ rows such that every $s$-sparse vector $x \in \mathbb{C}^{N}$ can be recovered from its measurement vector $y = Ax \in \mathbb{C}^{m}$ as the unique solution of minimal support.
\end{theorem}

Consider $A$ as the first $2s$ rows of the DFT matrix $M = (e^{2\pi i \frac{(l - 1)(k - 1)}{N}})_{k=l=1}^{N}$.
Applying $A$ to an $x \in \mathbb{C}^{N}$ computes its first $2s$ discrete Fourier coefficients.
If $x$ was $s$-sparse it can be derived from $y = Ax$.

Define the polynomial $p$ of degree $s$ as
\begin{equation*}
  p(t) := \prod_{k \in S} (1 - e^{-2\pi i \frac{k}{N}} e^{2\pi i \frac{t}{N}})
\end{equation*}
where $S = \supp x$.
Note that $t \in S \Rightarrow p(t) = 0$ and $t \not\in S \Rightarrow x(t) = 0$ where $x(t)$ is the functional equivalent to $x$'s vector form.
From this it follows that $p(t)x(t) = 0~\forall t \in [N]$.

Application of the DFT results in
\begin{equation*}
  0 = \widehat{p(\cdot)x(\cdot)}[j] = \sum_{k = 0}^{N - 1} p(k)x(k) e^{-2\pi i \frac{jk}{N}} \quad \text{for $0 \le j \le N - 1$}
\end{equation*}
By plugging in the inverse DFT of $p$ respectively $x$ you get
\begin{align}
  0 & = \sum_{k = 0}^{N - 1} \frac{1}{N^{2}} \left( \sum_{l = 0}^{N - 1} \hat{p}(l) e^{2\pi i \frac{kl}{N}} \right) \left( \sum_{m = 0}^{N - 1} \hat{x}(m) e^{2\pi i \frac{km}{N}} \right) e^{-2\pi i \frac{jk}{N}}\nonumber\\
    & = \sum_{l, m = 0}^{N - 1} \frac{1}{N^{2}} \hat{p}(l) \hat{x}(m) \underbrace{\sum_{k = 0}^{N - 1} e^{2\pi i \frac{k(l + m - j)}{N}}}_{= \begin{cases}
        0 & \text{if $l \not\equiv j - m \mod N$}\\
        N & \text{if $l \equiv j - m \mod N$}\\
      \end{cases}}\nonumber\\
    & = \frac{1}{N} \sum_{l = 0}^{N - 1} \hat{p}(l) \hat{x}(j - l) \label{eq:prony-sum}
\end{align}

Note that $p$ is a polynomial of degree $s$, thus it can be written as
\begin{equation*}
  p(t) = 1 + \sum_{l = 1}^{s} \alpha_{l} e^{2\pi i \frac{lt}{N}}
\end{equation*}
which shows that $\hat{p}(0) = N$ and $\hat{p}(l) = 0$ for $s < l < N$.
Equation \eqref{eq:prony-sum} becomes
\begin{equation}
  \begin{pmatrix}
    \hat{x}(s - 1) & \hat{x}(s - 2) & \hdots & \hat{x}(0)\\
    \hat{x}(s) & \hat{x}(s - 1) & \hdots & \hat{x}(1)\\
    \vdots & & & \\
    \hat{x}(2s - 2) & \hat{x}(2s - 3) & \hdots & \hat{x}(s - 1)
  \end{pmatrix}
  \begin{pmatrix}
    \hat{p}(1)\\
    \hat{p}(2)\\
    \vdots\\
    \hat{p}(s)
  \end{pmatrix}
  =
  -\begin{pmatrix}
    \hat{x}(s)\\
    \hat{x}(s + 1)\\
    \vdots\\
    \hat{x}(2s - 1)
  \end{pmatrix}
  \label{eq:prony-system}
\end{equation}
Recall that $\hat{x}(0), \dots, \hat{x}(2s - 1)$ are known because these are the measurements.
This system of equations cannot be solved in general, consider for example $x = e_{1}$ for which $\hat{x} = 1$ and all the matrix entries are $1$.
Hence, we can only find \emph{some} solution $[\hat{q}(1), \dots, \hat{q}(s)]$.
Extend $\hat{q}$ with our knowledge about $\hat{p}$ to $[N, \hat{q}(1), \dots, \hat{q}(s), 0, \dots, 0]$.
Now $x(t)q(t) = 0$ and $q$ vanishes on $S$.
Since $q$ cannot have more than $s$ zeros, the support of $x$ can be found by solving for the roots of $q$.

\begin{algorithm}[Prony's Method]
  \begin{enumerate}
  \item Find some solution $[\hat{q}(1), \dots, \hat{q}(s)]^{T}$ of Equation \eqref{eq:prony-system}
  \item Find the zeros of $q$, i.e. the support of $x$
  \item Solve overdetermined system given by columns of the DFT matrix $A$ corresponding to the support of $x$
  \end{enumerate}
\end{algorithm}

\begin{remark}
  \begin{enumerate}
  \item Prony's method is sensitive to noise
  \item Only works for sparse vectors, not for compressible ones
  \end{enumerate}
\end{remark}

\section{$l_{1}$-minimization and the Restricted Isometry Property}

\begin{remark}
  In a more general setup, no efficient algorithm like Prony's method is known.
  While uniqueness still holds under the above conditions there is no efficient algorithm to find the $l_{0}$-minimizer $\argmin_{Ax = y} ||x||_{0}$.
  The idea is to use the convex relaxation
  \begin{equation}
    x^{\#} = \argmin_{Ax = y} ||x||_{1} \label{eq:P1}
  \end{equation}
  instead.
\end{remark}

\begin{remark}
  Even though the above condition guarantees invertibility of submatrices, they may be badly conditioned.
  So in addition we will also require that subsets of $2s$ columns be well-conditioned.
\end{remark}

\begin{definition}[RIP]
  A matrix $A \in \mathbb{C}^{m \times N}$ has the \emph{restricted isometry property} (RIP) of order $s$ and level $\delta$ if for all $s$-sparse $x \in \mathbb{C}^{N}$
  \begin{equation*}
    (1 - \delta) ||x||_{2}^{2} \le ||Ax||_{2}^{2} \le (1 + \delta) ||x||_{2}^{2}
  \end{equation*}
  The \emph{restricted isometry constant} $\delta_{k}(A)$ is the smallest $\delta$ such that $A$ has the $(s, \delta)$-RIP.
\end{definition}

The goal of this chapter is to prove that one can recover the $l_{1}$-minimizer $x$ under the assumption of RIP.

We word in generalized setup:

\begin{description}
\item[stability] $x$ is not assumed to be sparse, just compressible
\item[robustness] the measurements are affected by noise: $y = Ax + \varepsilon$ with $||\varepsilon||_{2} \le \eta$.
\end{description}

The robustness requirements further complicates the problem statement and the solution is no longer an exact solution of $Ax = y$.
Instead we have to consider the problem
\begin{equation}
  x^{\#} = \argmin_{||Ax - y||_{2} \le \eta} ||x||_{1} \label{eq:P1eta}
\end{equation}

\begin{remark}
  We will see that the nullspace property is equivalent to the fact that Equation \eqref{eq:P1} recovers compressible vectors to a satisfactory degree, i.e. it links Equation \eqref{eq:P1} and RIP.
\end{remark}

\subsection{The Robust Nullspace Property}

\begin{definition}[Robust Nullspace-Property]
  The matrix $A \in \mathbb{C}^{m \times N}$ is said to have the \emph{robust nullspace property} (with respect to the $l_{2}$-norm) with constants $0 < \rho < 1$ and $\tau > 0$ relative to a set $S \subseteq [N]$ if
  \begin{equation*}
    ||v_{S}||_{1} \le \rho||v_{S^{C}}||_{1} + \tau ||Av||_{2} \qquad \text{for all $v \in \mathbb{C}^{N}$}
  \end{equation*}
  It is said to have the \emph{robust nullspace property of order $s$} with constants $0 < \rho < 1$ and $\tau > 0$ if it satisfies the robust nullspace property with constants $\rho, \tau$ relative to any set $S$ with $\card S \le s$.
\end{definition}

\begin{remark}
  The name nullspace property stems from its implication $||v_{S}|| \le \rho||v_{S^{C}}||_{1}$ for vectors $v \in \Ker A$.
\end{remark}

\begin{theorem}
  Suppose that a matrix $A \in \mathbb{C}^{m \times N}$ satisfies the robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x \in \mathbb{C}^{N}$, a solution $x^{\#}$ of Equation \eqref{eq:P1eta} with $y = Ax + \varepsilon$ and $||\varepsilon||_{2} < \eta$ approximates the vector $x$ with $l_{1}$-error
  \begin{equation*}
    ||x - x^{\#}||_{1} \le \frac{2(1 + \delta)}{1 - \delta}\sigma_{s}(x)_{1} + \frac{4\tau}{1 - \delta}\eta
  \end{equation*}
\end{theorem}

\begin{remark}
  This is exactly what we aimed for: When the noise is small , vectors $x$ close to sparse vectors are approximately recovered.
\end{remark}

\begin{theorem}
  The matrix $A \in \mathbb{C}^{m \times N}$ satisfies the robust nullspace property with constants $0 < \rho < 1$ and $\tau > 0$ relative to $S$ if and only if
  \begin{equation*}
    ||z - x||_{1} \le \frac{1 + \delta}{1 - \delta} \left( ||z||_{1} - ||x_{1}|| + 2 ||x_{S^{C}}||_{1} \right) + \frac{2\tau}{1 - \delta} ||A(z - x)||_{2}
  \end{equation*}
  for all vectors $x, z \in \mathbb{C}^{N}$.
\end{theorem}

\begin{lemma}
  Given a set $S \subset [N]$ and vectors $x, z \in \mathbb{C}^{N}$,
  \begin{equation*}
    ||(x - z)_{S^{C}}||_{1} \le ||z||_{1} - ||x||_{1} + ||(x - z)_{S}||_{1} + 2||x_{S^{C}}||_{1}
  \end{equation*}
\end{lemma}

\subsection{Restricted Isometries}

Recall that $A$ has the $(s, \delta)$-RIP if, for all $s$-sparse $x \in \mathbb{C}^{N}$,
\begin{equation*}
  (1 - \delta)||x||_{2}^{2} \le ||Ax||_{2}^{2} \le (1 + \delta) ||x||_{2}^{2}
\end{equation*}
Normalizing $x$ such that $||x||_{2} = 1$ yields
\begin{equation*}
  -\delta \le ||Ax||_{2}^{2} - 1 \le \delta \Leftrightarrow \big| ||Ax||_{2}^{2} - 1 \big| \le \delta
\end{equation*}
So we can determine the robust isometry constant as
\begin{align*}
  \delta_{s}(A) & = \sup_{\substack{x \in S^{N - 1}\\x~\text{$s$-sparse}}} \big| ||Ax||_{2}^{2} - 1 \big|\\
                & = \sup_{x \in S^{N - 1}} \sup_{\substack{S \subset [N]\\|S| = s}} \big| ||A_{S}x_{S}||_{2}^{2} - 1 \big|\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} \sup_{x \in S^{s - 1}} \big| ||A_{S}x||_{2}^{2} - 1 \big|\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} \sup_{x \in S^{s - 1}} x^{*}(A_{S}^{*}A_{S} - I)x\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} ||A_{S}^{*}A_{S} - I||
\end{align*}
where $||\cdot||$ is the spectral norm $||A|| = \sup_{||x||_{2} = 1} ||Ax||_{2}$.

\begin{proposition}
  Let $u, v \in \mathbb{C}^{N}$ be vectors with $||u||_{0} \le s$ and $||v||_{0} \le t$.
  If $\supp(u) \cap \supp(v) = \emptyset$, then
  \begin{equation*}
    |\langle Au, Av \rangle| \le \delta_{s+t}\,||u||_{2}\,||v||_{2}
  \end{equation*}
\end{proposition}

\subsection{The $l_{2}$-Robust Nullspace Property}

\begin{definition}[$l_{2}$-Robust Nullspace Property]
  A matrix $A \in \mathbb{C}^{m \times N}$ is said to have the $l_{2}$-robust nullspace property of order $s$ with respect to the $l_{2}$-norm with constants $0 < \rho < 1$ and $\tau > 0$ if, for any set $S \subset [N]$ with $\card S \le s$,
  \begin{equation*}
    ||v_{S}||_{2} \le \frac{\rho}{\sqrt{s}} ||v_{S^{C}}||_{1} + \tau ||Av||_{2}
  \end{equation*}
  for all $v \in \mathbb{C}^{N}$.
\end{definition}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $l_{2}$-robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x \in \mathbb{C}^{N}$, a solution $x^{\#}$ of \eqref{eq:P1eta} with $y = Ax + \varepsilon$ and $||\varepsilon||_{2} \le \eta$ approximates $x$ with $l_{2}$-error
  \begin{equation*}
    ||x - x^{\#}||_{2} \le \frac{C}{\sqrt{s}} \sigma_{s}(x)_{1} + D\eta
  \end{equation*}
  for some constants $C, D > 0$ depending only on $\rho$ and $\tau$.
\end{theorem}

\begin{theorem}
  Suppose $A \in \mathbb{C}^{m \times N}$ has the $l_{2}$-robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x, z \in \mathbb{C}^{N}$ and $1 \le p \le q$,
  \begin{equation*}
    ||z - x||_{2} \le \frac{C}{\sqrt{s}} (||z||_{1} - ||x||_{1} + 2 \sigma_{s}(x)_{1})
  \end{equation*}
  where $C := \frac{(1 + \delta)^{2}}{1 - \delta}$ and $D := \frac{(3 + \delta)^{\tau}}{1 - \delta}$.
\end{theorem}

\subsection{Connecting RIP and NSP}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $(2s, \delta)$-RIP for some $\delta < \frac{1}{3}$.
  Then $A$ has the $l_{2}$-robust NSP of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$ depending only on $\delta$.
\end{theorem}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $(2s, \delta)$-RIP for some $\delta < \frac{1}{3}$.
  Then, for any $x \in \mathbb{C}^{N}$ and $y \in \mathbb{C}^{m}$ with $||Ax - y||_{2} \le \eta$, a solution $x^{\#}$ of
  \begin{equation*}
    \min_{z \in \mathbb{C}^{N}} ||z||_{1} \quad \text{subject to $||Az - y|| \le \eta$}
  \end{equation*}
  approximates $x$ with error
  \begin{equation*}
    ||x - x^{\#}||_{2} \le \frac{c}{\sqrt{s}} \sigma_{2}(x)_{1} + D\eta
  \end{equation*}
  where the constants $C, D$ depend only on $\delta_{2s}(A)$.
\end{theorem}

\begin{remark}
  \begin{itemize}
  \item State-of-the-art result: Same holds when $\delta_{2s}(A) < \frac{4}{\sqrt{\pi}} \approx 0.6246$
  \item Sharp bounds are known for $\delta_{s}$:
    \begin{description}
    \item[$\delta_{s} < \frac{1}{3}$] $l_{1}$-minimization recovers sparse vectors
    \item[$\delta_{s} > \frac{1}{3}$] There is a matrix and a sparse vector such that $l_{1}$-minimization does not work
    \end{description}
  \item Using similar techniques, one obtains under the same assumptions
    \begin{equation*}
      ||x - x^{\#}||_{1} \le C \sigma_{s}(x)_{1} + D \sqrt{s} \eta
    \end{equation*}
  \end{itemize}
\end{remark}

\section{Basic Tools from Probability Theory and Functional Analysis}

In this whole section we have a probability space $(\Omega, \Sigma, \mathbb{P})$ where $\Omega$ is the sample space, $\Sigma$ is a $\sigma$-algebra of admissible events and $\mathbb{P}$ is a probability measure.

\subsection{Essentials from Probability}

\begin{lemma}[Union Bound]
  For a collection of events $B_{l} \in \Sigma, l = 1, \dots, n$, we have
  \begin{equation*}
    \Pr{\left( \cup_{l = 1}^{n} B_{l} \right)} \le \sum_{l = 1}^{n} \Pr{(B_{l})}
  \end{equation*}
\end{lemma}

\begin{definition}
  A \emph{random variable} $X$ is a real-valued measurable function on $(\Omega, \Sigma)$.
\end{definition}

\begin{definition}
  A random variable has a \emph{probability density function} (pdf) $\phi : \mathbb{R} \rightarrow \mathbb{R}_{+}$ if
  \begin{equation*}
    \mathbb{P}(a < X \le b) = \int_{a}^{b} \phi(t) \diff t
  \end{equation*}
\end{definition}

\begin{example}
  \begin{description}
  \item[Rademacher random variable] given by $\Pr{X = 1} = \Pr{X = -1} = \frac{1}{2}$
  \item[Standard normal random variable] $\Pr{X < a} = \int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^{2}}{2} \right) \diff x$
  \end{description}
\end{example}

\begin{definition}
  The \emph{expectation} or \emph{mean} of a random variable $X$ is denoted by
  \begin{equation*}
    \E X = \int_{\Omega} X(\omega) \diff \mathbb{P}(\omega)
  \end{equation*}

  If $X$ has a probability density function $\phi$
  \begin{equation*}
    \E g(x) = \int_{\Omega} g(t) \phi(f) \diff t
  \end{equation*}
  for any function $g : \mathbb{R} \rightarrow \mathbb{R}$.
\end{definition}

\begin{definition}
  $\E X^{p}$ and $\E |X|^{p}$ are called the \emph{$p$-th moment} respectively the \emph{$p$-th absolute moment} of $X$.
\end{definition}

\begin{lemma}[Basic but important facts]
  Let $X, Y$ be random variables on a common probability space.
  \begin{enumerate}
  \item $|\E X \cdot Y| \le \left( \E |X|^{p} \right)^{\frac{1}{p}} \cdot \left( \E |Y|^{q} \right)^{\frac{1}{q}}$ for $\frac{1}{p} + \frac{1}{q} = 1$ (Hölder's inequality)
  \item $|\E XY| \le \sqrt{\E |X|^{2} |Y|^{2}}$ (Cauchy-Schwarz inequality)
  \item $\left( \E |X + Y|^{p} \right)^{\frac{1}{p}} \le \left( \E |X|^{p} \right)^{\frac{1}{p}} + \left( \E |Y|^{p} \right)^{\frac{1}{p}}$ (Minkowski's inequality)
  \item If $\int_{\Sigma_{X} \times \Sigma_{Y}} |f(x, y)| \diff (\nu \otimes \mu)(x, y) < \infty$, then (Fubini)
    \begin{equation*}
      \int_{\Sigma_{X} \times \Sigma_{Y}} |f(x, y)| \diff (\nu \otimes \mu)(x, y) = \int_{\Sigma_{X}} \int_{\Sigma_{Y}} f(x, y) \diff \mu(y) \diff \nu(x) = \int_{\Sigma_{Y}} \int_{\Sigma_{X}} f(x, y) \diff \nu(x) \diff \mu(y)
    \end{equation*}
  \end{enumerate}
\end{lemma}

\begin{proposition}
  The $p$-th absolute moments of a random variable $X$ can be expressed as
  \begin{equation*}
    \E |X|^{p} = p \int_{0}^{\infty} \Pr{|X| \ge t} t^{p - 1} \diff t, p > 0
  \end{equation*}
\end{proposition}

\begin{definition}
  The \emph{tail} of the probability distribution of a random variable $X$ is the function $r \mapsto \Pr{|X| \ge t}$.
\end{definition}

\begin{theorem}[Markov's inequality]
  Let $X$ be a random variable. Then
  \begin{equation*}
    \Pr{|X| \ge t} \le \frac{\E |X|}{t}
  \end{equation*}
  for all $t > 0$.
\end{theorem}

\subsection{Moments and Tails}

This uses the $\Gamma$-function defined by
\begin{equation*}
  \Gamma(x) = \int_{0}^{\infty} t^{x - 1} e^{-t} \diff t
\end{equation*}

\begin{theorem}[Stirling's formula]
  \begin{equation*}
    \Gamma(x) = \sqrt{2\pi} x^{x - \frac{1}{2}} e^{-x} \exp\left( \frac{\mu(x)}{12x} \right)
  \end{equation*}
  for $x > 0$ and an unknown function $0 \le \mu(x) \le 1$.
\end{theorem}

\begin{lemma}[Functional Equation of the $\Gamma$-function]
  \begin{equation*}
    \Gamma(x + 1) = x \Gamma(x)
  \end{equation*}
\end{lemma}

\begin{proposition}
  Suppose that a random variable $Z$ satisfies for some $\gamma > 0$
  \begin{equation*}
    \Pr{|Z| \ge e^{\frac{1}{\gamma}} \alpha u} \le \beta e^{-\frac{u^{\gamma}}{\gamma}}
  \end{equation*}
  for all $u > 0$.
  Then for $p > 0$,
  \begin{equation*}
    \E |Z|^{p} \le \beta \alpha^{p} (e \gamma)^{\frac{p}{\gamma}} \Gamma\left( \frac{p}{\gamma} + 1 \right)
  \end{equation*}
  As a consequence, for $p \ge 1$,
  \begin{equation*}
    \left( \E |Z|^{p} \right)^{\frac{1}{p}} \le C_{1} \alpha \left( C_{2,\gamma} \beta \right)^{\frac{1}{p}} p^{\frac{1}{\gamma}}
  \end{equation*}
  for all $p \ge 1$, where $C_{1} = e^{\frac{1}{2e}} \approx 1.2$ and $C_{2,\gamma} = \sqrt{\frac{2p}{\gamma}} e^{\frac{\gamma}{12}}$.
\end{proposition}

\subsection{Collections of Random Variables and Random Vectors}

\begin{definition}
  A \emph{random vector} $X = [X_{1}, \dots, X_{n}]^{T} \in \mathbb{R}^{n}$ is a collection of $n$ random variables on a common probability space $(\Omega, \Sigma, \mathbb{P})$.
  Its expectation is the vector $\E X = [\E X_{1}, \dots, \E X_{n}] \in \mathbb{R}^{n}$.
  $X$ has a \emph{joint probability distribution} if there exists a function $\phi : \mathbb{R}^{n} \rightarrow \mathbb{R}_{+}$ such that for any measurable domain $D \subset \mathbb{R}^{n}$
  \begin{equation*}
    \Pr{X \in D} = \int \phi(t_{1}, \dots, t_{n}) \diff t_{1} \dots \diff t_{n}
  \end{equation*}
\end{definition}

\begin{definition}
  A collection of random variables is \emph{(stochastically) independent} if, for all $t_{1}, \dots, t_{n} \in \mathbb{R}$,
  \begin{equation*}
    \Pr{X_{1} \le t_{1}, \dots, X_{n} \le t_{n}} = \prod_{l = 1}^{n} \Pr{X_{l} \le t_{l}}
  \end{equation*}
\end{definition}

\begin{lemma}
  A independent collection of random variables satisfies
  \begin{equation*}
    \E \left( \prod_{l = 1}^{n} X_{l} \right) = \prod_{l = 1}^{n} \E X_{l}
  \end{equation*}

  If they have a joint probability density function $\phi$, then
  \begin{equation*}
    \phi(t_{1}, \dots, t_{n}) = \prod_{l = 1}^{n} \phi_{l}(t_{l})
  \end{equation*}
  where $\phi_{l}$ is the pdf of $X_{l}$.
\end{lemma}

\begin{definition}
  A collection of $m$ random vectors $X_{1} \in \mathbb{R}^{n_{1}}, \dots, X_{m} \in \mathbb{R}^{n_{m}}$ are independent if for any collection of measurable sets $A_{l} \subset \mathbb{R}^{n_{l}}, l \in [m]$
  \begin{equation*}
    \Pr{X_{1} \in A_{1}, \dots, X_{m} \in A_{m}} = \prod_{l = 1}^{m} \Pr{X_{l} \in A_{l}}
  \end{equation*}
\end{definition}

\begin{definition}
  A collection of independent random vectors $X_{1}, \dots, X_{m}$ that have the same distribution are called independent identically distributed.
\end{definition}

\begin{definition}
  If $X'$ is independent from $X$ and has the same distribution, we say that $X'$ is an \emph{independent copy} of $X$.
\end{definition}

\subsection{Subgaussian Random Variables and Random Vectors}

\begin{definition}
  A random variable $X$ is called \emph{subgaussian} if there exist constants $\beta, \kappa > 0$ such that
  \begin{equation*}
    \Pr{|X| \ge t} \le \beta e^{-\kappa t^{2}} \qquad \text{for all $t > 0$}
  \end{equation*}
  It is called \emph{subexponential} if
  \begin{equation*}
    \Pr{|x| \ge t} \le \beta e^{-\kappa t} \qquad \text{for all $t > 0$}
  \end{equation*}
\end{definition}

\begin{lemma}
  $X$ is subgaussian if and only if $X^{2}$ is subexponential.
\end{lemma}

\begin{example}
  Standard normal and Rademacher random variables are both subgaussian.
\end{example}

\begin{proposition}
  Let $X$ be a random variable.
  \begin{enumerate}
  \item If $X$ is subgaussian, then there exist constants $c > 0$, $C > 1$ such that $\E \exp(c X^{2}) \le C$
  \item If $\E \exp(c X^{2}) \le C$ for some constants $c, C > 0$, then $X$ is subgaussian.
    More precisely, $\Pr{|X| > t} \le C e^{-c t^{2}}$.
  \item If $X$ is subgaussian with constants $\beta, \kappa$, then $\left( \E |X|^{p} \right)^{\frac{1}{p}} \le \tilde{C} \kappa^{-\frac{1}{2}} \beta^{\frac{1}{p}} p^{\frac{1}{2}}$ for all $p |ge 1$, where $\tilde{C}$ is an absolute constant.
  \end{enumerate}
\end{proposition}

\begin{proposition}
  \label{proposition:subgaussian}
  Let $X$ be a random variable.
  \begin{enumerate}
  \item If $X$ is subgaussian with parameters $\beta, \kappa$ and $\E X = 0$, then there exists a constant $c$ (depending only on $\beta$ and $\kappa$) such that $\E \exp(\theta X) \le \exp(c \theta^{2})$ for all $\theta \in \mathbb{R}$.
  \item Conversely, if $\E \exp(\theta X) \le \exp(c \theta^{2})$ holds for all $\theta \in \mathbb{R}$, then $\E X = 0$ and $X$ is subgaussian with parameters $\beta = 2$ and $\kappa = \frac{1}{4c}$.
  \end{enumerate}
\end{proposition}

\begin{remark}
  This constant $c$ is also called the \emph{subgaussian parameter} of $X$.
\end{remark}

\begin{theorem}
  Let $X_{1}, \dots, X_{m}$ be a sequence of independent, mean-zero subgaussian random variables with subgaussian parameter $c$.
  Let $a \in \mathbb{R}^{m}$ be some vector.
  Then $Z := \sum_{l = 1}^{m} a_{l} X_{l}$ is subgaussian with parameter $c||a||_{2}^{2}$, i.e.
  \begin{equation*}
    \E \exp(\theta Z) \le \exp\left( c||a||_{2}^{2} \theta^{2} \right)
  \end{equation*}
  and thus
  \begin{equation*}
    \Pr{\left| \sum_{l = 1}^{m} a_{l} X_{l} \right| \ge t} \le 2 \exp\left( -\frac{t^{2}}{4c||a||_{2}^{2}} \right)
  \end{equation*}
  for all $t > 0$.
\end{theorem}

\begin{definition}
  Let $Y$ by a random vector on $\mathbb{R}^{N}$.
  \begin{enumerate}
  \item If $\E |\langle Y, x \rangle|^{2} = ||x||_{2}^{2}$ for all $x \in \mathbb{R}^{N}$, then $Y$ is called \emph{isotropic}.
  \item If, for all $x \in \mathbb{R}^{N}$ with $||x||_{2} = 1$, the random variable $\langle Y, x \rangle$ is subgaussian with parameter bounded by $c$ independent of $x$, that is,
    \begin{equation*}
      \E \exp(\theta \langle Y, x \rangle) \le \exp(c \theta^{2}) \qquad \forall \theta \in \mathbb{R}
    \end{equation*}
    then $Y$ is called a \emph{subgaussian random vector}.
  \end{enumerate}
\end{definition}

Again $c$ is called the subgaussian parameter of $Y$.

\begin{remark}
  A random vector $X$ with independent entries of mean $0$ and second moment $1$ is isotropic and subgaussian with a parameter independent of the dimension.
  The inverse is not true in general, i.e. an isotropic subgaussian random vector need not have independent entries.
\end{remark}

\begin{lemma}
  Let $Y \in \mathbb{R}^{N}$ be a random vector with independent, mean zero, subgaussian entries $Y_{l}$ with $\E Y_{l}^{2} = 1$ and subgaussian parameter $c$.
  Then $Y$ is an isotropic subgaussian vector with parameter $c$.
\end{lemma}

\subsection{The Covering Argument}

\begin{definition}
  Let $T$ be a subset of a metric space $(X, d)$.
  For $t > 0$, the \emph{convering number} $\mathcal{N}(T, d, t)$ is the smallest $N \in \mathbb{N}$ such that $T$ can be covered with balls
  \begin{equation*}
    B(x_{l}, t) = \{ x \in X, d(x, x_{l}) \le t \}, x_{l} \in T, l = 1, \dots, N
  \end{equation*}
  i.e. $T \subseteq \cup_{l = 1}^{} B(x_{l}, t)$.
  The set of points $\{ x_{1}, \dots, x_{N} \}$ is called a \emph{$t$-covering}.
  The \emph{packing number} $\mathcal{P}(T, d, t)$, for $t > 0$, is the maximal $P \in \mathbb{Z}$ such that there exist $x_{l} \in T, l \in \{ 1, \dots, P \}$, which are \emph{$t$-separated}, i.e. $d(x_{l}, x_{k}) > t$ for all $k, l = 1, \dots, P, k \le l$.
  If $(X, ||.||)$ is a vector space we also write $\mathcal{T, ||.||, t}$ and $\mathcal{P}(T, ||.||, t)$.
\end{definition}

\begin{lemma}[Properties]
  Let $(X, d)$ be a metric space, $S, T \subset X$, $\alpha > 0$.
  Then
  \begin{enumerate}
  \item $\mathcal{N}(S \cup T, d, t) \le \mathcal{N}(S, d, t) + \mathcal{N}(T, d, t)$
  \item $\mathcal{N}(T, \alpha d, t) = \mathcal{N}\left( T, d, \frac{t}{\alpha} \right)$
  \item If $X = \mathbb{R}^{n}$ and $d$ is induced by a norm $||.||$, then $\mathcal{N}(\alpha T, d, t) = \mathcal{N}(T, d, \alpha^{-1} t)$
  \item If $d'$ is another metric that satisfies $d'(x, y) \le d(x, y)$ for all $x, y \in T$, then $\mathcal{N}(T, d', t) \le \mathcal{N}(T, d, t)$
  \end{enumerate}
  The same relations hold for $\mathcal{P}$.
\end{lemma}

\begin{lemma}
  Let $T$ be a subset of a metric space $(X, d)$ and $t > 0$.
  Then
  \begin{equation*}
    \mathcal{P}(T, d, 2t) \le \mathcal{N}(T, d, t) \le \mathcal{P}(T, d, t)
  \end{equation*}
\end{lemma}

\begin{proposition}
  Let $||.||$ be a norm on $\mathbb{R}^{n}$ and let $U$ be a subset of the unit ball $B = \{ x \in \mathbb{R}^{n}, ||x|| \le 1 \}$.
  Then the packing and convering numbers satisfy
  \begin{equation*}
    \mathcal{N}(U, ||.||, t) \le \mathcal{P}(U, ||.||, t) \le \left( 1 + \frac{2}{t} \right)^{n}
  \end{equation*}
\end{proposition}

\begin{theorem}[Covering argument]
  Let $B$ be an $N \times n$-matrix and let $\mathcal{N}$ be a $t$-covering of $S^{n - 1}$ for some $t \in [0, 1)$.
  \begin{enumerate}
  \item $\max_{x \in \mathcal{N}} ||Bx||_{2} \le ||B|| \le (1 - t)^{-1} \max_{x \in \mathcal{N}} ||Bx||_{2}$
  \item If furthermore $N = n$ and $B$ is symmetric
    \begin{equation*}
      \max_{x \in \mathcal{N}} |\langle Bx, x \rangle| \le ||B|| \le (1 - 2t)^{-1} \max_{x \in \mathcal{N}} \max_{x \in \mathcal{N}} |\langle Bx, x \rangle|
    \end{equation*}
  \end{enumerate}
\end{theorem}

\subsection{Bernstein's inequality}

\begin{theorem}[Bernstein's inequality]
  Let $X_{1}, \dots, X_{M}$ be independent centered subexponential random variables with parameters $(\beta, \kappa)$.
  Then for all $t > 0$ and an absolute constant $C$
  \begin{equation*}
    \Pr{\left| \sum_{k = 1}^{M} X_{l} \right| > t} \le 2\exp\left( -\frac{\kappa^{2} t^{2}}{4(2\beta M + \kappa t)} \right)
  \end{equation*}
\end{theorem}

\section{Examples of RIP matrices: Subgaussian Matrices}

\begin{definition}
  Let $A$ be an $m \times N$ random matrix.
  \begin{itemize}
  \item If the entries of $A$ are independent Rademacher variables, i.e. $\Pr{X = 1} = \Pr{X = -1} = \frac{1}{2}$, then $A$ is called a \emph{Bernoulli random matrix}.
  \item If the entries of $A$ are independent standard normal random variables, then $A$ is called a \emph{Gaussian random matrix}.
  \item If the entries of $A$ are independent mean-zero subgaussian random variables of variance $1$ with the same constants $\beta, \kappa$, i.e. $\Pr{|A_{i,j}| \ge t} \le \beta e^{-\kappa t^{2}}$ for all $t > 0$, $i \in [m], j \in [N]$, then $A$ is called a \emph{subgaussian random matrix}.
  \end{itemize}
\end{definition}

\begin{remark}
  In the last case then entries are required to be independent but not identically distributed.
\end{remark}

\begin{theorem}
  let $A$ be an $m \times N$ subgaussian rando matrix.
  Then there exists a constant $C > 0$ (depending only on the subgaussian parameters $\beta$, $\kappa$) such that the restricted isometry constant of $\frac{1}{\sqrt{m}}A$ satisfies $\delta_{s} \le \delta$ with probability at least $1 - \varepsilon$ provided
  \begin{equation*}
    m \ge C \delta^{-2} \left( s \log\left( \frac{eN}{s} \right) + \log(2\varepsilon^{-1}) \right)
  \end{equation*}
\end{theorem}

\begin{remark}
  \begin{itemize}
  \item Setting $\varepsilon = \exp\left( -\delta^{2} \frac{m}{2C} \right)$ yields a simple condition $m \ge 2C\delta^{-2} s \log\left( \frac{eN}{s} \right)$ with corresponding success probability $\ge 1 - \exp\left( -\delta^{2} \frac{m}{2C} \right)$.
  \item The restricted isometry property for any $s$ entails that norms on $1$-sparse vectors must be preserved.
    This means that each column of the RIP matrix must have norm approximately $1$, which is why the normalization constant is $\frac{1}{\sqrt{m}}$.
  \end{itemize}
\end{remark}

\begin{lemma}
  Let $A$ be an $m \times N$ random matrix whose rows are independent, isotropic, subgaussian random vectors with the same subgaussian parameter $c$, i.e. $\E\left[ \exp(\lambda \langle A_{i}, x \rangle) \right] \le \exp(c \lambda^{2})$ for all $\lambda \in \mathbb{R}$, $||x||_{2} = 1$ where $A_{i}$ is the $i$-th row of $A$.
  Then, for all $x \in \mathbb{R}^{N}$ and every $t \in (0, 1)$,
  \begin{equation*}
    \Pr{\big| m^{-1} ||Ax||_{2}^{2} - ||x||_{2}^{2} \big| \ge t||x||_{2}^{2}} \le 2\exp(-\tau t^{2} m)
  \end{equation*}
  where $\tau$ depends only on $c$.
\end{lemma}

\begin{theorem}
  Suppose that an $m \times N$ random matrix $A$ is drawn according to a probability distribution for which the concentration inequality
  \begin{equation*}
    \Pr{\big| ||Ax||_{2}^{2} - ||x||_{2}^{2} \big| > t||x||_{2}^{2}} \le 2\exp(-\tilde{c}t^{2}m)
  \end{equation*}
  holds for all $t \in (0, 1)$ and $x \in \mathbb{R}^{N}$.
  If for $\delta, \varepsilon \in (0, 1)$
  \begin{equation*}
    m \ge C \delta^{-2} \left( s \log\left( \frac{N}{s} \right) + \log(\varepsilon^{-1}) \right)
  \end{equation*}
  where $C$ only depends on $\tilde{c}$, then with probability at least $1 - \varepsilon$ the restricted isometry property constant $\delta_{s}$ of $A$ satisfies $\delta_{s} \le \delta$.
\end{theorem}

\section{Advanced Probabilistic Tools}

\subsection{Rademacher Sums and Symmetrization}

\begin{definition}
  A \emph{Rademacher sum} is of the form $\sum_{l = 1}^{M} \varepsilon_{l} x_{l}$ where $\varepsilon_{l}$ are independent Rademacher variables and $x_{l}$ scalars, vectors or matrices.
\end{definition}

The proof strategy developed in this chapter has two steps:
\begin{enumerate}
\item Estimate sum of independent random variables via a Rademacher sum (Symmetrization)
\item Estimate Rademacher sum using standard tools
\end{enumerate}

\subsubsection{Introducing Rademacher variables into the sum}

\begin{lemma}[Symmetrization]
  Assume that $\xi = (\xi_{l})_{l = 1}^{M}$ is a sequence of independent random vectors in a finite-dimensional vector space $V$ with norm $||\cdot||$.
  Let $F : V \rightarrow \mathbb{R}$ be a convex function.
  Then
  \begin{equation*}
    \E F\left( \sum_{l = 1}^{M} \left( \xi_{l} - \E \xi_{l} \right) \right) \le \E F\left( 2 \sum_{l = 1}^{M} \varepsilon_{l} \xi_{l} \right),
  \end{equation*}
  where $(\varepsilon_{l})$ is a Rademacher sequence independent of $(\xi_{l})$.
  In particular, for $1 \le p < \infty$,
  \begin{equation*}
    \left( \E \Big|\Big| \sum_{l = 1}^{M} \xi_{l} - \E \xi_{l} \Big|\Big|^{p} \right)^{\frac{1}{p}} \le 2 \left( \E \Big|\Big| \sum_{l = 1}^{M} \varepsilon_{l} \xi_{l} \Big|\Big|^{p} \right)^{\frac{1}{p}}
  \end{equation*}
\end{lemma}

\subsubsection{Estimating moments and tails of Rademacher sums}

using Khintchin's inequality

\begin{theorem}
  Let $a \in \mathbb{C}^{M}$ and $(\varepsilon_{1}, \dots, \varepsilon_{M})$ be a Rademacher sequence.
  Then, for all $n \in \mathbb{N}$,
  \begin{equation*}
    \E \Big| \sum_{l = 1}^{M} \varepsilon_{l} a_{l} \Big|^{2n} \le \frac{(2n)!}{2^{n}n!} ||a||_{2}^{2n}
  \end{equation*}
\end{theorem}

\begin{lemma}[Multinomial Theorem]
  For $m, n \in \mathbb{N}$ and $x_{1}, \dots, x_{m} \in \mathbb{C}$, one has
  \begin{equation*}
    \left( \sum_{l = 1}^{m} x_{l} \right)^{n} = \sum_{k_{1} + \dots + k_{m} = n} \frac{n!}{k_{1}! \cdots k_{m}!} \prod_{j = 1}^{m} x_{j}^{k_{j}}
  \end{equation*}
\end{lemma}

\begin{corollary}[Hoeffding's Inequality]
  \label{corollary:hoeffding}
  Let $a \in \mathbb{C}^{M}$ and $(\varepsilon_{l})_{l = 1}^{M}$ be a Rademacher sequence.
  Then there is a constant $C$ such that for all $u > 0$
  \begin{equation*}
    \Pr{\Big| \sum_{l = 1}^{M} \varepsilon_{l} a_{l} \Big| \ge ||a||_{2} u} \le 2 \exp(-C u^{2})
  \end{equation*}
\end{corollary}

\begin{remark}
  Using a more refined analysis (Stirling's formula) one obtains the optimal constant $C = \frac{1}{2}$.
\end{remark}

\subsection{Dudley's Inequality}

We want to estimate suprema of stochastic processes.

\begin{definition}
  A \emph{stochastic process} is a collection $(X_{t})_{t \in T}$ of random variables for some set $T$.
\end{definition}

In this section $T$ will be the set of sparse vectors and $X_{t} = |\langle t, (A^{*}A - I)t \rangle|$ or a random matrix.
Then $\sup_{t} X_{t}$ will be the restricted isometry constant.

\begin{itemize}
\item Estimate $\E \sup_{t} X_{t}$
\item Estimate deviation $\Pr{|\sup_{t} X_{t} - \E \sup_{t} X_{t}| > \varepsilon}$
\end{itemize}

Step 1 is based on Dudley's inequality, while step 2 requires technical tools based on concentration measure.

In this chapter we assume that the process is centered, i.e. $\E X_{t} = 0$ for all $t \in T$.

Associated to the process $(X_{t})_{t \in T}$ we define the pseudo-metric
\begin{equation}
  d(s, t) := \left( \E |X_{s} - X_{t}|^{2} \right)^{\frac{1}{2}} \label{eq:pseudometric}
\end{equation}

\begin{definition}
  A \emph{pseudo-metric} $d$ has to properties
  \begin{itemize}
  \item $d(x, y) = d(y, x)$
  \item $d(x, z) \le d(x, y) + d(y, z)$
  \end{itemize}
  The third condition in the definition of a metric $d(x, y) = 0 \Leftrightarrow x = y$ does not necessarily hold.
\end{definition}

\begin{definition}
  A centered stochastic process $(X_{t})_{t \in T}$ is called \emph{subgaussian} with parameter $C > 0$ if
  \begin{equation*}
    \E \exp(\theta(X_{s} - X_{t})) \le \exp(\theta^{2} C d(s, t)^{2}), \qquad s, t \in T, \theta > 0
  \end{equation*}
  with $d$ being the pseudo-metric defined in Equation \eqref{eq:pseudometric}.
\end{definition}

\begin{example}
  A process $(X_{t})_{t \in T}$ is called a centered Gaussian process if, for every finite collection $t_{1}, \dots, t_{n} \in T$, the random vector $(X_{t_{1}}, \dots, X_{t_{n}})$ is a mean-zero Gaussian rando vector.
  Then $X_{t} - X_{s}$ is a univariate Gaussian random variable with $\E X_{t} - X_{s} = 0$ and, by definition, variance $\E |X_{t} - X_{s}|^{2} = d(s, t)^{2}$.
  Thus $\frac{X_{t} - X_{s}}{d(s, t)}$ is a standard normal and hence subgaussian and we have by Proposition \ref{proposition:subgaussian}
  \begin{equation*}
    \E \exp\left( \tilde{\theta} \frac{X_{t} - X_{s}}{d(s, t)} \right) \le \exp(c \tilde{\theta}^{2}) \quad \text{for all $\tilde{\theta} \in \mathbb{R}$}
  \end{equation*}
  which for $\tilde{\theta} = \theta d(s, t)$ implies that $X_{t}$ is a subgaussian process.
\end{example}

\begin{example}
  A \emph{Rademacher process} has the form
  \begin{equation*}
    X_{t} = \sum_{j = 1}^{M} \varepsilon_{j} x_{j}(t),
  \end{equation*}
  where $(\varepsilon_{l})_{l = 1}^{M}$ is a Rademacher sequence and $x_{j} : T \rightarrow \mathbb{R}, j \in [M]$ are arbitrary functions.
  $X_t$ is centered as $\E \varepsilon_{j} = 0$ for all $j$.
  The pseudo-metric \eqref{eq:pseudometric} becomes
  \begin{equation*}
    d(s, t) = \sqrt{\E |X_{t} - X_{s}|^{2}} = \sqrt{\E \Big| \sum_{j = 1}^{M} \varepsilon_{j} (x_{j}^{t} - x_{j}^{s}) \Big|^{2}} = \left( \sum_{j = 1}^{M} \left( x_{j}(t) - x_{j}(s) \right)^{2} \right)^{\frac{1}{2}} = ||x(t) - x(s)||_{2}
  \end{equation*}
  On the other hand Hoeffding's inequality, Corollary \ref{corollary:hoeffding}, implies that
  \begin{equation*}
    \Pr{\Big| \sum_{l = 1}^{M} \varepsilon_{l} (x_{l}(t) - x_{l}(s)) \Big| > ||x(t) - x(s)||_{2} u} \le 2\exp(-Cu^{2})
  \end{equation*}
  Thus the subgaussian parameter of $X_{t} - X_{s}$ is bounded by $\tilde{c}||x(t) - x(s)||_{2} = \tilde{c}d(s, t)$ where $\tilde{c}$ is an absolute constant, again by Proposition \ref{proposition:subgaussian}, which implies that $X_{t}$ is a subgaussian process.
\end{example}

\begin{theorem}[Dudley's Inequality]
  Let $(X_{t})_{t \in T}$ be a centered real-valued subgaussian process with associated pseudo-metric $d$ and subgaussian parameter $c$.
  Then
  \begin{itemize}
  \item $\E \sup_{t \in T} X_{t} \le 8\sqrt{c} \int_{0}^{\frac{\Delta(T)}{2}} \sqrt{\log(\mathcal{N}(T, d, u))} \diff u$
  \item $\E \sup_{t \in T} |X_{t}| \le 8\sqrt{c} \int_{0}^{\frac{\Delta(T)}{2}} \sqrt{\log(2\mathcal{N}(T, d, u))} \diff u$
  \end{itemize}
  where $\Delta(T) = \sup_{t \in T} \sqrt{\E |X_{t}|^{2}}$ is the radius of $T$ with respect to $d$.
\end{theorem}

\begin{remark}
  If $(X_{t})_{t \in T}$ is a Gaussian process, T is a subset of a finite dimensional space and $d$ is induced by a norm, these bounds are known to be sharp up to logarithmic factors in the dimension.
\end{remark}

\section{Random Sampling in Bounded Orthogonal Systems}

Recall that our main motivation was to recover images from undersampled linear measurements, the prime example of which is Magnetic Resonance Imaging (MRI).
There we measure an image $f \in L^{2}([0, 1]^{2})$ by
\begin{equation*}
  f(s, t) = \sum_{k, l} \hat{f}(k, l) \cdot e^{2\pi i (ks + lt)}
\end{equation*}
where $\hat{f}$ are the MRI measurements which correspond to the Fourier coefficients
\begin{equation*}
  \hat{f}(k, l) = \int f(s, t) \cdot e^{-2\pi i (ks + lt)} \diff s \diff t
\end{equation*}
This image is discretized as $f \in \mathbb{C}^{N \times N}$.
DFT measurements of $(\mathcal{F}f)$ provide a discrete approximation of the MRI measurement.
From this one can recover the original by applying the adjoint DFT matrix due to $\mathcal{F}^{*}\mathcal{F} = I$.
The question is: Can we recover approximately sparse signals from a small subset of their DFT measurements?

The resulting matrix is a collection of random samples of rows of $\mathcal{F}$.

So far we have seen that
\begin{enumerate}
\item $2s$ measurements allows for reconstructions (uniqueness), but there is no efficient algorithm
\item If a matrix $A$ has the Restricted Isometry Property of order $s$ and level $\delta < \frac{1}{3}$, then $s$-sparse vectors can be reconstructed via $l_{1}$-minimization
\item Subgaussian matrices have the RIP with high probability for embedding dimension $m = O\left(\delta^{-1}s\log\left( \frac{N}{s} \right)\right)$.
\end{enumerate}
We need an extension of the last point: Partial Fourier matrices have the RIP for embedding dimension $m \ge ?$.
Again we use randomness to select a collection of frequencies to construct the partial Fourier matrix.
But we will examine this as a special case of sampling a Bounded Orthonormal System.

\subsection{Bounded Orthogonal Systems}

\begin{definition}
  Let $D \subset \mathbb{R}^{d}$ be endowed with a probability measure $\nu$.
  Further, let $\Phi = \{ \phi_{1}, \dots, \phi_{N} \}$ be an orthonormal system of complex valued functions on $D$, that is, for $j, k \in [N]$
  \begin{equation*}
    \int_{D} \phi_{j}(t) \overline{\phi_{k}(t)} \diff \nu(t) = \delta_{jk} = \begin{cases}
      0 & \text{if $j \ne k$}\\
      1 & \text{if $j = k$}
    \end{cases}
  \end{equation*}
  Then $\Phi$ is called an \emph{bounded orthonormal system} (BOS) with constant $K$ if
  \begin{equation*}
    ||\phi_{j}||_{\infty} := \sup_{t \in D} |\phi_{j}(t)| \le K \quad \text{for all $j \in [N]$}
  \end{equation*}
\end{definition}

\begin{definition}
  The \emph{random matrix} of embedding dimension $m$ associated to a BOS consists of $m$ rows $(\phi_{1}(t_{l}), \dots, \phi_{N}(t_{l})), l = 1..m$ where the $t_{l}$ are independently drawn according to $\nu$.
\end{definition}

\subsection{Restricted Isometry Property for Bounded Orthogonal Systems}

\begin{theorem}
  Let $D \subset \mathbb{R}^{d}$ and let $\{ \phi_{j} \}_{j = 1}^{N}$ be a BOS on $D$ with associated probability measure $\nu$ and constant $k \ge 1$.
  Let $\delta \in (0, 1)$.
  Then exists $C > 0$ such that the random matrix associated to the BOS
  \begin{equation*}
    \tilde{A} = \frac{1}{\sqrt{m}} \left( \phi_{k}(t_{l}) \right)_{l = 1, k = 1}^{m, N}, t_{l} \sim \nu~\text{i.i.d.}
  \end{equation*}
  satisfies
  \begin{equation*}
    \E \delta_{s}(\tilde{A}) \le \delta
  \end{equation*}
  if $m$ satisfies
  \begin{equation*}
    \frac{m}{\log(9m)} \ge C \delta^{-2} K^{2} s \log(4s)\log(8N)
  \end{equation*}
\end{theorem}

\begin{lemma}[Noncommutative Rudelson's Lemma]
  Let $x_{1}, \dots, x_{m} \in \mathbb{C}^{N}$ with $||x_{l}||_{\infty} \le k$ for all $l \in [m]$.
  Then, for $s \le m$,
  \begin{equation*}
    \E \Big|\Big|\Big| \sum_{l = 1}^{m} \varepsilon_{l} x_{l}x_{l}^{*} \Big|\Big|\Big|_{s} \le C_{1} K \sqrt{s} \log(4s) \sqrt{\log(8N) \log(9m)} \sqrt{\Big|\Big|\Big| \sum_{l = 1}^{m} x_{l}x_{l}^{*} \Big|\Big|\Big|}
  \end{equation*}
  where $C_{1}$ is an absolute constant.
\end{lemma}

\end{document}
