\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% Define the page margin
\usepackage[margin=3cm]{geometry}

% Better typography (font rendering)
\usepackage{microtype}

% Math environments and macros
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

% Define \includegraphics to include graphics
\usepackage{graphicx}

% Syntax highlighting
\usepackage{minted}

% Set global minted options
\setminted{linenos, autogobble, mathescape, numbersep=5pt}

% Import the comment environment for orgtbl-mode
\usepackage{comment}

% Do not indent paragraphs
\usepackage{parskip}

\newtheoremstyle{thmstyle}
  {6pt} % Space above
  {6pt} % Space below
  {} % Body font
  {} % Indent amount
  {\bfseries} % Theorem head font
  {.} % Punctuation after theorem head
  {.5em} % Space after theorem head
  {} % Theorem head spec (can be left empty, meaning `normal')
\theoremstyle{thmstyle}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}
\newtheorem{algorithm}{Algorithm}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\argmin}{arg\,min}

\newcommand{\diff}{~\mathrm{d}}
\newcommand{\E}{\mathbb{E}~}
\renewcommand{\Pr}[1]{\mathbb{P}\left( #1 \right)}

\title{Compressed Compressed Sensing Script}
\author{Marten Lienen}
\date{}

\begin{document}

\maketitle

This will list only definitions, lemmata and theorems but no proofs.

\section{Introduction}

\subsection{Imaging}

\subsection{Machine Learning}

\subsection{Sparsity and Compressibility}

\begin{notation}
  \begin{description}
  \item[$p$-Norm]
    \begin{equation*}
      ||x||_{p} = \left( \sum_{j = 1}^{N} |x_{j}|^{p} \right)^{\frac{1}{p}}, \quad 0 < p < \infty
    \end{equation*}
  \item[$\infty$-quasinorm]
    \begin{equation*}
      ||x||_{\infty} = \sup_{j} |x_{j}|
    \end{equation*}
  \item[Sequence from $1$ to $N$]
    \begin{equation*}
      [N] = \{ 1, \dots, N \}
    \end{equation*}
  \end{description}
\end{notation}

\begin{definition}[sparsity, support]
  For a vector $x \in \mathbb{R}^{N}$ or $\mathbb{C}^{N}$ its \emph{support} is defined as
  \begin{equation*}
    \supp x = \{ k : x_{k} \ne 0 \}
  \end{equation*}
  $x$ is \emph{$k$-sparse} if $\supp x \le k$.
  Furthermore, denote $||x||_{0} := |\supp x|$; with some abuse of notation we speak of the $0$-norm (as $||x||_{p} \rightarrow_{p \rightarrow 0} ||x||0$).
\end{definition}

\begin{definition}[error of best $s$-term approximation]
  The error of the best $s$-term approximation of a vector $x \in \mathbb{R}^{N}$ or $\mathbb{C}^{N}$ is
  \begin{equation*}
    \sigma_{s}(x)_{p} := \inf \{ ||x - z||_{p} : z~\text{is $s$-sparse} \}
  \end{equation*}
\end{definition}

\begin{definition}
  A vector $x$ is called \emph{compressible} if $\sigma_{s}(x)_{p}$ decays quickly in $s$.
\end{definition}

\begin{proposition}[Stechkin]
  For any $q > p > 0$ and any $x \in \mathbb{C}^{N}$,
  \begin{equation*}
    \sigma_{s}(x)_{q} \le \frac{1}{s^{\frac{1}{p} - \frac{1}{q}}} ||x||_{p}
  \end{equation*}
\end{proposition}

\begin{definition}
  The \emph{non-increasing rearrangement} of $x \in \mathbb{C}^{N}$ is the vector $x^{*} \in \mathbb{R}_{+}^{N}$ for which
  \begin{equation*}
    x_{1}^{*} \ge x_{2}^{*} \ge \dots \ge x_{N}^{*} \ge 0
  \end{equation*}
  and there is a permutation $\pi : [N] \rightarrow [N]$ with $x_{j}^{*} = |x_{\pi(j)}|$ for all $j \in [N]$.
\end{definition}

\begin{remark}
  Using convex analysis the constant can be refined to obtain
  \begin{equation*}
    \sigma_{s}(x)_{q} \le \frac{c_{p,q}}{s^{\frac{1}{p} - \frac{1}{q}}} ||x||_{p}
  \end{equation*}
  where
  \begin{equation*}
    c_{p,q} = \left[ \left( \frac{p}{q} \right)^{\frac{p}{q}} \left( 1 - \frac{p}{1} \right)^{1 - \frac{p}{q}} \right]^{\frac{1}{p}}
  \end{equation*}
  For $p = 1, q = 2$ this yields $\sigma_{s}(x)_{2} \le \frac{1}{2\sqrt{s}} ||x||_{1}$.
\end{remark}

\subsection{The Compressed Sensing Problem}

Solve $Ax = y$ for $x$ knowing that $x$ is sparse (or compressible).
Does this problem even have a unique solution?
Not always, for example choose $A$ as a subset of rows of $I$.

\begin{theorem}
  Given $A \in \mathbb{C}^{m \times N}$, the following properties are equivalent.
  \begin{itemize}
  \item every $s$-sparse vector $x \in \mathbb{C}^{N}$ is the unique $s$-sparse solution of $Az = Ax$, that is, if $Ax = Az$ and both $x$ ans $z$ are $s$-sparse, then $x = z$.
  \item The null space $\Ker A$ does not contain any $2s$-sparse vector other than the zero vector, that is, $(\Ker A) \cap \{ z \in \mathbb{C}^{N} : ||z||_{0} \le 2s \} = \{ 0 \}$.
  \item For every $S \subset [N]$ with $\card(s) \le 2s$, the submatrix $A_{S}$ is injective.
  \item Every set of $2s$ columns of $A$ is linearly independent.
  \end{itemize}
\end{theorem}

\begin{theorem}
  For any $N \ge 2s$, there exists a measurement matrix $A \in \mathbb{C}^{m \times N}$ with $m = 2s$ rows such that every $s$-sparse vector $x \in \mathbb{C}^{N}$ can be recovered from its measurement vector $y = Ax \in \mathbb{C}^{m}$ as the unique solution of minimal support.
\end{theorem}

Consider $A$ as the first $2s$ rows of the DFT matrix $M = (e^{2\pi i \frac{(l - 1)(k - 1)}{N}})_{k=l=1}^{N}$.
Applying $A$ to an $x \in \mathbb{C}^{N}$ computes its first $2s$ discrete Fourier coefficients.
If $x$ was $s$-sparse it can be derived from $y = Ax$.

Define the polynomial $p$ of degree $s$ as
\begin{equation*}
  p(t) := \prod_{k \in S} (1 - e^{-2\pi i \frac{k}{N}} e^{2\pi i \frac{t}{N}})
\end{equation*}
where $S = \supp x$.
Note that $t \in S \Rightarrow p(t) = 0$ and $t \not\in S \Rightarrow x(t) = 0$ where $x(t)$ is the functional equivalent to $x$'s vector form.
From this it follows that $p(t)x(t) = 0~\forall t \in [N]$.

Application of the DFT results in
\begin{equation*}
  0 = \widehat{p(\cdot)x(\cdot)}[j] = \sum_{k = 0}^{N - 1} p(k)x(k) e^{-2\pi i \frac{jk}{N}} \quad \text{for $0 \le j \le N - 1$}
\end{equation*}
By plugging in the inverse DFT of $p$ respectively $x$ you get
\begin{align}
  0 & = \sum_{k = 0}^{N - 1} \frac{1}{N^{2}} \left( \sum_{l = 0}^{N - 1} \hat{p}(l) e^{2\pi i \frac{kl}{N}} \right) \left( \sum_{m = 0}^{N - 1} \hat{x}(m) e^{2\pi i \frac{km}{N}} \right) e^{-2\pi i \frac{jk}{N}}\nonumber\\
    & = \sum_{l, m = 0}^{N - 1} \frac{1}{N^{2}} \hat{p}(l) \hat{x}(m) \underbrace{\sum_{k = 0}^{N - 1} e^{2\pi i \frac{k(l + m - j)}{N}}}_{= \begin{cases}
        0 & \text{if $l \not\equiv j - m \mod N$}\\
        N & \text{if $l \equiv j - m \mod N$}\\
      \end{cases}}\nonumber\\
    & = \frac{1}{N} \sum_{l = 0}^{N - 1} \hat{p}(l) \hat{x}(j - l) \label{eq:prony-sum}
\end{align}

Note that $p$ is a polynomial of degree $s$, thus it can be written as
\begin{equation*}
  p(t) = 1 + \sum_{l = 1}^{s} \alpha_{l} e^{2\pi i \frac{lt}{N}}
\end{equation*}
which shows that $\hat{p}(0) = N$ and $\hat{p}(l) = 0$ for $s < l < N$.
Equation \eqref{eq:prony-sum} becomes
\begin{equation}
  \begin{pmatrix}
    \hat{x}(s - 1) & \hat{x}(s - 2) & \hdots & \hat{x}(0)\\
    \hat{x}(s) & \hat{x}(s - 1) & \hdots & \hat{x}(1)\\
    \vdots & & & \\
    \hat{x}(2s - 2) & \hat{x}(2s - 3) & \hdots & \hat{x}(s - 1)
  \end{pmatrix}
  \begin{pmatrix}
    \hat{p}(1)\\
    \hat{p}(2)\\
    \vdots\\
    \hat{p}(s)
  \end{pmatrix}
  =
  -\begin{pmatrix}
    \hat{x}(s)\\
    \hat{x}(s + 1)\\
    \vdots\\
    \hat{x}(2s - 1)
  \end{pmatrix}
  \label{eq:prony-system}
\end{equation}
Recall that $\hat{x}(0), \dots, \hat{x}(2s - 1)$ are known because these are the measurements.
This system of equations cannot be solved in general, consider for example $x = e_{1}$ for which $\hat{x} = 1$ and all the matrix entries are $1$.
Hence, we can only find \emph{some} solution $[\hat{q}(1), \dots, \hat{q}(s)]$.
Extend $\hat{q}$ with our knowledge about $\hat{p}$ to $[N, \hat{q}(1), \dots, \hat{q}(s), 0, \dots, 0]$.
Now $x(t)q(t) = 0$ and $q$ vanishes on $S$.
Since $q$ cannot have more than $s$ zeros, the support of $x$ can be found by solving for the roots of $q$.

\begin{algorithm}[Prony's Method]
  \begin{enumerate}
  \item Find some solution $[\hat{q}(1), \dots, \hat{q}(s)]^{T}$ of Equation \eqref{eq:prony-system}
  \item Find the zeros of $q$, i.e. the support of $x$
  \item Solve overdetermined system given by columns of the DFT matrix $A$ corresponding to the support of $x$
  \end{enumerate}
\end{algorithm}

\begin{remark}
  \begin{enumerate}
  \item Prony's method is sensitive to noise
  \item Only works for sparse vectors, not for compressible ones
  \end{enumerate}
\end{remark}

\section{$l_{1}$-minimization and the Restricted Isometry Property}

\begin{remark}
  In a more general setup, no efficient algorithm like Prony's method is known.
  While uniqueness still holds under the above conditions there is no efficient algorithm to find the $l_{0}$-minimizer $\argmin_{Ax = y} ||x||_{0}$.
  The idea is to use the convex relaxation
  \begin{equation}
    x^{\#} = \argmin_{Ax = y} ||x||_{1} \label{eq:P1}
  \end{equation}
  instead.
\end{remark}

\begin{remark}
  Even though the above condition guarantees invertibility of submatrices, they may be badly conditioned.
  So in addition we will also require that subsets of $2s$ columns be well-conditioned.
\end{remark}

\begin{definition}[RIP]
  A matrix $A \in \mathbb{C}^{m \times N}$ has the \emph{restricted isometry property} (RIP) of order $s$ and level $\delta$ if for all $s$-sparse $x \in \mathbb{C}^{N}$
  \begin{equation*}
    (1 - \delta) ||x||_{2}^{2} \le ||Ax||_{2}^{2} \le (1 + \delta) ||x||_{2}^{2}
  \end{equation*}
  The \emph{restricted isometry constant} $\delta_{k}(A)$ is the smallest $\delta$ such that $A$ has the $(s, \delta)$-RIP.
\end{definition}

The goal of this chapter is to prove that one can recover the $l_{1}$-minimizer $x$ under the assumption of RIP.

We word in generalized setup:

\begin{description}
\item[stability] $x$ is not assumed to be sparse, just compressible
\item[robustness] the measurements are affected by noise: $y = Ax + \varepsilon$ with $||\varepsilon||_{2} \le \eta$.
\end{description}

The robustness requirements further complicates the problem statement and the solution is no longer an exact solution of $Ax = y$.
Instead we have to consider the problem
\begin{equation}
  x^{\#} = \argmin_{||Ax - y||_{2} \le \eta} ||x||_{1} \label{eq:P1eta}
\end{equation}

\begin{remark}
  We will see that the nullspace property is equivalent to the fact that Equation \eqref{eq:P1} recovers compressible vectors to a satisfactory degree, i.e. it links Equation \eqref{eq:P1} and RIP.
\end{remark}

\subsection{The Robust Nullspace Property}

\begin{definition}[Robust Nullspace-Property]
  The matrix $A \in \mathbb{C}^{m \times N}$ is said to have the \emph{robust nullspace property} (with respect to the $l_{2}$-norm) with constants $0 < \rho < 1$ and $\tau > 0$ relative to a set $S \subseteq [N]$ if
  \begin{equation*}
    ||v_{S}||_{1} \le \rho||v_{S^{C}}||_{1} + \tau ||Av||_{2} \qquad \text{for all $v \in \mathbb{C}^{N}$}
  \end{equation*}
  It is said to have the \emph{robust nullspace property of order $s$} with constants $0 < \rho < 1$ and $\tau > 0$ if it satisfies the robust nullspace property with constants $\rho, \tau$ relative to any set $S$ with $\card S \le s$.
\end{definition}

\begin{remark}
  The name nullspace property stems from its implication $||v_{S}|| \le \rho||v_{S^{C}}||_{1}$ for vectors $v \in \Ker A$.
\end{remark}

\begin{theorem}
  Suppose that a matrix $A \in \mathbb{C}^{m \times N}$ satisfies the robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x \in \mathbb{C}^{N}$, a solution $x^{\#}$ of Equation \eqref{eq:P1eta} with $y = Ax + \epsilon$ and $||\epsilon||_{2} < \eta$ approximates the vector $x$ with $l_{1}$-error
  \begin{equation*}
    ||x - x^{\#}||_{1} \le \frac{2(1 + \delta)}{1 - \delta}\sigma_{s}(x)_{1} + \frac{4\tau}{1 - \delta}\eta
  \end{equation*}
\end{theorem}

\begin{remark}
  This is exactly what we aimed for: When the noise is small , vectors $x$ close to sparse vectors are approximately recovered.
\end{remark}

\begin{theorem}
  The matrix $A \in \mathbb{C}^{m \times N}$ satisfies the robust nullspace property with constants $0 < \rho < 1$ and $\tau > 0$ relative to $S$ if and only if
  \begin{equation*}
    ||z - x||_{1} \le \frac{1 + \delta}{1 - \delta} \left( ||z||_{1} - ||x_{1}|| + 2 ||x_{S^{C}}||_{1} \right) + \frac{2\tau}{1 - \delta} ||A(z - x)||_{2}
  \end{equation*}
  for all vectors $x, z \in \mathbb{C}^{N}$.
\end{theorem}

\begin{lemma}
  Given a set $S \subset [N]$ and vectors $x, z \in \mathbb{C}^{N}$,
  \begin{equation*}
    ||(x - z)_{S^{C}}||_{1} \le ||z||_{1} - ||x||_{1} + ||(x - z)_{S}||_{1} + 2||x_{S^{C}}||_{1}
  \end{equation*}
\end{lemma}

\subsection{Restricted Isometries}

Recall that $A$ has the $(s, \delta)$-RIP if, for all $s$-sparse $x \in \mathbb{C}^{N}$,
\begin{equation*}
  (1 - \delta)||x||_{2}^{2} \le ||Ax||_{2}^{2} \le (1 + \delta) ||x||_{2}^{2}
\end{equation*}
Normalizing $x$ such that $||x||_{2} = 1$ yields
\begin{equation*}
  -\delta \le ||Ax||_{2}^{2} - 1 \le \delta \Leftrightarrow \big| ||Ax||_{2}^{2} - 1 \big| \le \delta
\end{equation*}
So we can determine the robust isometry constant as
\begin{align*}
  \delta_{s}(A) & = \sup_{\substack{x \in S^{N - 1}\\x~\text{$s$-sparse}}} \big| ||Ax||_{2}^{2} - 1 \big|\\
                & = \sup_{x \in S^{N - 1}} \sup_{\substack{S \subset [N]\\|S| = s}} \big| ||A_{S}x_{S}||_{2}^{2} - 1 \big|\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} \sup_{x \in S^{s - 1}} \big| ||A_{S}x||_{2}^{2} - 1 \big|\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} \sup_{x \in S^{s - 1}} x^{*}(A_{S}^{*}A_{S} - I)x\\
                & = \sup_{\substack{S \subset [N]\\|S| = s}} ||A_{S}^{*}A_{S} - I||
\end{align*}
where $||\cdot||$ is the spectral norm $||A|| = \sup_{||x||_{2} = 1} ||Ax||_{2}$.

\begin{proposition}
  Let $u, v \in \mathbb{C}^{N}$ be vectors with $||u||_{0} \le s$ and $||v||_{0} \le t$.
  If $\supp(u) \cap \supp(v) = \emptyset$, then
  \begin{equation*}
    |\langle Au, Av \rangle| \le \delta_{s+t}\,||u||_{2}\,||v||_{2}
  \end{equation*}
\end{proposition}

\subsection{The $l_{2}$-Robust Nullspace Property}

\begin{definition}[$l_{2}$-Robust Nullspace Property]
  A matrix $A \in \mathbb{C}^{m \times N}$ is said to have the $l_{2}$-robust nullspace property of order $s$ with respect to the $l_{2}$-norm with constants $0 < \rho < 1$ and $\tau > 0$ if, for any set $S \subset [N]$ with $\card S \le s$,
  \begin{equation*}
    ||v_{S}||_{2} \le \frac{\rho}{\sqrt{s}} ||v_{S^{C}}||_{1} + \tau ||Av||_{2}
  \end{equation*}
  for all $v \in \mathbb{C}^{N}$.
\end{definition}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $l_{2}$-robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x \in \mathbb{C}^{N}$, a solution $x^{\#}$ of \eqref{eq:P1eta} with $y = Ax + \epsilon$ and $||\epsilon||_{2} \le \eta$ approximates $x$ with $l_{2}$-error
  \begin{equation*}
    ||x - x^{\#}||_{2} \le \frac{C}{\sqrt{s}} \sigma_{s}(x)_{1} + D\eta
  \end{equation*}
  for some constants $C, D > 0$ depending only on $\rho$ and $\tau$.
\end{theorem}

\begin{theorem}
  Suppose $A \in \mathbb{C}^{m \times N}$ has the $l_{2}$-robust nullspace property of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$.
  Then, for any $x, z \in \mathbb{C}^{N}$ and $1 \le p \le q$,
  \begin{equation*}
    ||z - x||_{2} \le \frac{C}{\sqrt{s}} (||z||_{1} - ||x||_{1} + 2 \sigma_{s}(x)_{1})
  \end{equation*}
  where $C := \frac{(1 + \delta)^{2}}{1 - \delta}$ and $D := \frac{(3 + \delta)^{\tau}}{1 - \delta}$.
\end{theorem}

\subsection{Connecting RIP and NSP}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $(2s, \delta)$-RIP for some $\delta < \frac{1}{3}$.
  Then $A$ has the $l_{2}$-robust NSP of order $s$ with constants $0 < \rho < 1$ and $\tau > 0$ depending only on $\delta$.
\end{theorem}

\begin{theorem}
  Suppose that $A \in \mathbb{C}^{m \times N}$ has the $(2s, \delta)$-RIP for some $\delta < \frac{1}{3}$.
  Then, for any $x \in \mathbb{C}^{N}$ and $y \in \mathbb{C}^{m}$ with $||Ax - y||_{2} \le \eta$, a solution $x^{\#}$ of
  \begin{equation*}
    \min_{z \in \mathbb{C}^{N}} ||z||_{1} \quad \text{subject to $||Az - y|| \le \eta$}
  \end{equation*}
  approximates $x$ with error
  \begin{equation*}
    ||x - x^{\#}||_{2} \le \frac{c}{\sqrt{s}} \sigma_{2}(x)_{1} + D\eta
  \end{equation*}
  where the constants $C, D$ depend only on $\delta_{2s}(A)$.
\end{theorem}

\begin{remark}
  \begin{itemize}
  \item State-of-the-art result: Same holds when $\delta_{2s}(A) < \frac{4}{\sqrt{\pi}} \approx 0.6246$
  \item Sharp bounds are known for $\delta_{s}$:
    \begin{description}
    \item[$\delta_{s} < \frac{1}{3}$] $l_{1}$-minimization recovers sparse vectors
    \item[$\delta_{s} > \frac{1}{3}$] There is a matrix and a sparse vector such that $l_{1}$-minimization does not work
    \end{description}
  \item Using similar techniques, one obtains under the same assumptions
    \begin{equation*}
      ||x - x^{\#}||_{1} \le C \sigma_{s}(x)_{1} + D \sqrt{s} \eta
    \end{equation*}
  \end{itemize}
\end{remark}

\section{Basic Tools from Probability Theory and Functional Analysis}

In this whole section we have a probability space $(\Omega, \Sigma, \mathbb{P})$ where $\Omega$ is the sample space, $\Sigma$ is a $\sigma$-algebra of admissible events and $\mathbb{P}$ is a probability measure.

\subsection{Essentials from Probability}

\begin{lemma}[Union Bound]
  For a collection of events $B_{l} \in \Sigma, l = 1, \dots, n$, we have
  \begin{equation*}
    \Pr{\left( \cup_{l = 1}^{n} B_{l} \right)} \le \sum_{l = 1}^{n} \Pr{(B_{l})}
  \end{equation*}
\end{lemma}

\begin{definition}
  A \emph{random variable} $X$ is a real-valued measurable function on $(\Omega, \Sigma)$.
\end{definition}

\begin{definition}
  A random variable has a \emph{probability density function} (pdf) $\phi : \mathbb{R} \rightarrow \mathbb{R}_{+}$ if
  \begin{equation*}
    \mathbb{P}(a < X \le b) = \int_{a}^{b} \phi(t) \diff t
  \end{equation*}
\end{definition}

\begin{example}
  \begin{description}
  \item[Rademacher random variable] given by $\Pr{X = 1} = \Pr{X = -1} = \frac{1}{2}$
  \item[Standard normal random variable] $\Pr{X < a} = \int_{-\infty}^{a} \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^{2}}{2} \right) \diff x$
  \end{description}
\end{example}

\begin{definition}
  The \emph{expectation} or \emph{mean} of a random variable $X$ is denoted by
  \begin{equation*}
    \E X = \int_{\Omega} X(\omega) \diff \mathbb{P}(\omega)
  \end{equation*}

  If $X$ has a probability density function $\phi$
  \begin{equation*}
    \E g(x) = \int_{\Omega} g(t) \phi(f) \diff t
  \end{equation*}
  for any function $g : \mathbb{R} \rightarrow \mathbb{R}$.
\end{definition}

\begin{definition}
  $\E X^{p}$ and $\E |X|^{p}$ are called the \emph{$p$-th moment} respectively the \emph{$p$-th absolute moment} of $X$.
\end{definition}

\begin{lemma}[Basic but important facts]
  Let $X, Y$ be random variables on a common probability space.
  \begin{enumerate}
  \item $|\E X \cdot Y| \le \left( \E |X|^{p} \right)^{\frac{1}{p}} \cdot \left( \E |Y|^{q} \right)^{\frac{1}{q}}$ for $\frac{1}{p} + \frac{1}{q} = 1$ (HÃ¶lder's inequality)
  \item $|\E XY| \le \sqrt{\E |X|^{2} |Y|^{2}}$ (Cauchy-Schwarz inequality)
  \item $\left( \E |X + Y|^{p} \right)^{\frac{1}{p}} \le \left( \E |X|^{p} \right)^{\frac{1}{p}} + \left( \E |Y|^{p} \right)^{\frac{1}{p}}$ (Minkowski's inequality)
  \item If $\int_{\Sigma_{X} \times \Sigma_{Y}} |f(x, y)| \diff (\nu \otimes \mu)(x, y) < \infty$, then (Fubini)
    \begin{equation*}
      \int_{\Sigma_{X} \times \Sigma_{Y}} |f(x, y)| \diff (\nu \otimes \mu)(x, y) = \int_{\Sigma_{X}} \int_{\Sigma_{Y}} f(x, y) \diff \mu(y) \diff \nu(x) = \int_{\Sigma_{Y}} \int_{\Sigma_{X}} f(x, y) \diff \nu(x) \diff \mu(y)
    \end{equation*}
  \end{enumerate}
\end{lemma}

\begin{proposition}
  The $p$-th absolute moments of a random variable $X$ can be expressed as
  \begin{equation*}
    \E |X|^{p} = p \int_{0}^{\infty} \Pr{|X| \ge t} t^{p - 1} \diff t, p > 0
  \end{equation*}
\end{proposition}

\begin{definition}
  The \emph{tail} of the probability distribution of a random variable $X$ is the function $r \mapsto \Pr{|X| \ge t}$.
\end{definition}

\begin{theorem}[Markov's inequality]
  Let $X$ be a random variable. Then
  \begin{equation*}
    \Pr{|X| \ge t} \le \frac{\E |X|}{t}
  \end{equation*}
  for all $t > 0$.
\end{theorem}

\subsection{Moments and Tails}

This uses the $\Gamma$-function defined by
\begin{equation*}
  \Gamma(x) = \int_{0}^{\infty} t^{x - 1} e^{-t} \diff t
\end{equation*}

\begin{theorem}[Stirling's formula]
  \begin{equation*}
    \Gamma(x) = \sqrt{2\pi} x^{x - \frac{1}{2}} e^{-x} \exp\left( \frac{\mu(x)}{12x} \right)
  \end{equation*}
  for $x > 0$ and an unknown function $0 \le \mu(x) \le 1$.
\end{theorem}

\begin{lemma}[Functional Equation of the $\Gamma$-function]
  \begin{equation*}
    \Gamma(x + 1) = x \Gamma(x)
  \end{equation*}
\end{lemma}

\begin{proposition}
  Suppose that a random variable $Z$ satisfies for some $\gamma > 0$
  \begin{equation*}
    \Pr{|Z| \ge e^{\frac{1}{\gamma}} \alpha u} \le \beta e^{-\frac{u^{\gamma}}{\gamma}}
  \end{equation*}
  for all $u > 0$.
  Then for $p > 0$,
  \begin{equation*}
    \E |Z|^{p} \le \beta \alpha^{p} (e \gamma)^{\frac{p}{\gamma}} \Gamma\left( \frac{p}{\gamma} + 1 \right)
  \end{equation*}
  As a consequence, for $p \ge 1$,
  \begin{equation*}
    \left( \E |Z|^{p} \right)^{\frac{1}{p}} \le C_{1} \alpha \left( C_{2,\gamma} \beta \right)^{\frac{1}{p}} p^{\frac{1}{\gamma}}
  \end{equation*}
  for all $p \ge 1$, where $C_{1} = e^{\frac{1}{2e}} \approx 1.2$ and $C_{2,\gamma} = \sqrt{\frac{2p}{\gamma}} e^{\frac{\gamma}{12}}$.
\end{proposition}

\subsection{Collections of Random Variables and Random Vectors}

\begin{definition}
  A \emph{random vector} $X = [X_{1}, \dots, X_{n}]^{T} \in \mathbb{R}^{n}$ is a collection of $n$ random variables on a common probability space $(\Omega, \Sigma, \mathbb{P})$.
  Its expectation is the vector $\E X = [\E X_{1}, \dots, \E X_{n}] \in \mathbb{R}^{n}$.
  $X$ has a \emph{joint probability distribution} if there exists a function $\phi : \mathbb{R}^{n} \rightarrow \mathbb{R}_{+}$ such that for any measurable domain $D \subset \mathbb{R}^{n}$
  \begin{equation*}
    \Pr{X \in D} = \int \phi(t_{1}, \dots, t_{n}) \diff t_{1} \dots \diff t_{n}
  \end{equation*}
\end{definition}

\begin{definition}
  A collection of random variables is \emph{(stochastically) independent} if, for all $t_{1}, \dots, t_{n} \in \mathbb{R}$,
  \begin{equation*}
    \Pr{X_{1} \le t_{1}, \dots, X_{n} \le t_{n}} = \prod_{l = 1}^{n} \Pr{X_{l} \le t_{l}}
  \end{equation*}
\end{definition}

\begin{lemma}
  A independent collection of random variables satisfies
  \begin{equation*}
    \E \left( \prod_{l = 1}^{n} X_{l} \right) = \prod_{l = 1}^{n} \E X_{l}
  \end{equation*}

  If they have a joint probability density function $\phi$, then
  \begin{equation*}
    \phi(t_{1}, \dots, t_{n}) = \prod_{l = 1}^{n} \phi_{l}(t_{l})
  \end{equation*}
  where $\phi_{l}$ is the pdf of $X_{l}$.
\end{lemma}

\begin{definition}
  A collection of $m$ random vectors $X_{1} \in \mathbb{R}^{n_{1}}, \dots, X_{m} \in \mathbb{R}^{n_{m}}$ are independent if for any collection of measurable sets $A_{l} \subset \mathbb{R}^{n_{l}}, l \in [m]$
  \begin{equation*}
    \Pr{X_{1} \in A_{1}, \dots, X_{m} \in A_{m}} = \prod_{l = 1}^{m} \Pr{X_{l} \in A_{l}}
  \end{equation*}
\end{definition}

\begin{definition}
  A collection of independent random vectors $X_{1}, \dots, X_{m}$ that have the same distribution are called independent identically distributed.
\end{definition}

\begin{definition}
  If $X'$ is independent from $X$ and has the same distribution, we say that $X'$ is an \emph{independent copy} of $X$.
\end{definition}

\subsection{Subgaussian Random Variables and Random Vectors}

\begin{definition}
  A random variable $X$ is called \emph{subgaussian} if there exist constants $\beta, \kappa > 0$ such that
  \begin{equation*}
    \Pr{|X| \ge t} \le \beta e^{-\kappa t^{2}} \qquad \text{for all $t > 0$}
  \end{equation*}
  It is called \emph{subexponential} if
  \begin{equation*}
    \Pr{|x| \ge t} \le \beta e^{-\kappa t} \qquad \text{for all $t > 0$}
  \end{equation*}
\end{definition}

\begin{lemma}
  $X$ is subgaussian if and only if $X^{2}$ is subexponential.
\end{lemma}

\begin{example}
  Standard normal and Rademacher random variables are both subgaussian.
\end{example}

\begin{proposition}
  Let $X$ be a random variable.
  \begin{enumerate}
  \item If $X$ is subgaussian, then there exist constants $c > 0$, $C > 1$ such that $\E \exp(c X^{2}) \le C$
  \item If $\E \exp(c X^{2}) \le C$ for some constants $c, C > 0$, then $X$ is subgaussian.
    More precisely, $\Pr{|X| > t} \le C e^{-c t^{2}}$.
  \item If $X$ is subgaussian with constants $\beta, \kappa$, then $\left( \E |X|^{p} \right)^{\frac{1}{p}} \le \tilde{C} \kappa^{-\frac{1}{2}} \beta^{\frac{1}{p}} p^{\frac{1}{2}}$ for all $p |ge 1$, where $\tilde{C}$ is an absolute constant.
  \end{enumerate}
\end{proposition}

\begin{proposition}
  Let $X$ be a random variable.
  \begin{enumerate}
  \item If $X$ is subgaussian with parameters $\beta, \kappa$ and $\E X = 0$, then there exists a constant $c$ (depending only on $\beta$ and $\kappa$) such that $\E \exp(\theta X) \le \exp(c \theta^{2})$ for all $\theta \in \mathbb{R}$.
  \item Conversely, if $\E \exp(\theta X) \le \exp(c \theta^{2})$ holds for all $\theta \in \mathbb{R}$, then $\E X = 0$ and $X$ is subgaussian with parameters $\beta = 2$ and $\kappa = \frac{1}{4c}$.
  \end{enumerate}
\end{proposition}

\begin{remark}
  This constant $c$ is also called the \emph{subgaussian parameter} of $X$.
\end{remark}

\begin{theorem}
  Let $X_{1}, \dots, X_{m}$ be a sequence of independent, mean-zero subgaussian random variables with subgaussian parameter $c$.
  Let $a \in \mathbb{R}^{m}$ be some vector.
  Then $Z := \sum_{l = 1}^{m} a_{l} X_{l}$ is subgaussian with parameter $c||a||_{2}^{2}$, i.e.
  \begin{equation*}
    \E \exp(\theta Z) \le \exp\left( c||a||_{2}^{2} \theta^{2} \right)
  \end{equation*}
  and thus
  \begin{equation*}
    \Pr{\left| \sum_{l = 1}^{m} a_{l} X_{l} \right| \ge t} \le 2 \exp\left( -\frac{t^{2}}{4c||a||_{2}^{2}} \right)
  \end{equation*}
  for all $t > 0$.
\end{theorem}

\begin{definition}
  Let $Y$ by a random vector on $\mathbb{R}^{N}$.
  \begin{enumerate}
  \item If $\E |\langle Y, x \rangle|^{2} = ||x||_{2}^{2}$ for all $x \in \mathbb{R}^{N}$, then $Y$ is called \emph{isotropic}.
  \item If, for all $x \in \mathbb{R}^{N}$ with $||x||_{2} = 1$, the random variable $\langle Y, x \rangle$ is subgaussian with parameter bounded by $c$ independent of $x$, that is,
    \begin{equation*}
      \E \exp(\theta \langle Y, x \rangle) \le \exp(c \theta^{2}) \qquad \forall \theta \in \mathbb{R}
    \end{equation*}
    then $Y$ is called a \emph{subgaussian random vector}.
  \end{enumerate}
\end{definition}

Again $c$ is called the subgaussian parameter of $Y$.

\begin{remark}
  A random vector $X$ with independent entries of mean $0$ and second moment $1$ is isotropic and subgaussian with a parameter independent of the dimension.
  The inverse is not true in general, i.e. an isotropic subgaussian random vector need not have independent entries.
\end{remark}

\begin{lemma}
  Let $Y \in \mathbb{R}^{N}$ be a random vector with independent, mean zero, subgaussian entries $Y_{l}$ with $\E Y_{l}^{2} = 1$ and subgaussian parameter $c$.
  Then $Y$ is an isotropic subgaussian vector with parameter $c$.
\end{lemma}

\subsection{The Covering Argument}

\begin{definition}
  Let $T$ be a subset of a metric space $(X, d)$.
  For $t > 0$, the \emph{convering number} $\mathcal{N}(T, d, t)$ is the smallest $N \in \mathbb{N}$ such that $T$ can be covered with balls
  \begin{equation*}
    B(x_{l}, t) = \{ x \in X, d(x, x_{l}) \le t \}, x_{l} \in T, l = 1, \dots, N
  \end{equation*}
  i.e. $T \subseteq \cup_{l = 1}^{} B(x_{l}, t)$.
  The set of points $\{ x_{1}, \dots, x_{N} \}$ is called a \emph{$t$-covering}.
  The \emph{packing number} $\mathcal{P}(T, d, t)$, for $t > 0$, is the maximal $P \in \mathbb{Z}$ such that there exist $x_{l} \in T, l \in \{ 1, \dots, P \}$, which are \emph{$t$-separated}, i.e. $d(x_{l}, x_{k}) > t$ for all $k, l = 1, \dots, P, k \le l$.
  If $(X, ||.||)$ is a vector space we also write $\mathcal{T, ||.||, t}$ and $\mathcal{P}(T, ||.||, t)$.
\end{definition}

\begin{lemma}[Properties]
  Let $(X, d)$ be a metric space, $S, T \subset X$, $\alpha > 0$.
  Then
  \begin{enumerate}
  \item $\mathcal{N}(S \cup T, d, t) \le \mathcal{N}(S, d, t) + \mathcal{N}(T, d, t)$
  \item $\mathcal{N}(T, \alpha d, t) = \mathcal{N}\left( T, d, \frac{t}{\alpha} \right)$
  \item If $X = \mathbb{R}^{n}$ and $d$ is induced by a norm $||.||$, then $\mathcal{N}(\alpha T, d, t) = \mathcal{N}(T, d, \alpha^{-1} t)$
  \item If $d'$ is another metric that satisfies $d'(x, y) \le d(x, y)$ for all $x, y \in T$, then $\mathcal{N}(T, d', t) \le \mathcal{N}(T, d, t)$
  \end{enumerate}
  The same relations hold for $\mathcal{P}$.
\end{lemma}

\begin{lemma}
  Let $T$ be a subset of a metric space $(X, d)$ and $t > 0$.
  Then
  \begin{equation*}
    \mathcal{P}(T, d, 2t) \le \mathcal{N}(T, d, t) \le \mathcal{P}(T, d, t)
  \end{equation*}
\end{lemma}

\begin{proposition}
  Let $||.||$ be a norm on $\mathbb{R}^{n}$ and let $U$ be a subset of the unit ball $B = \{ x \in \mathbb{R}^{n}, ||x|| \le 1 \}$.
  Then the packing and convering numbers satisfy
  \begin{equation*}
    \mathcal{N}(U, ||.||, t) \le \mathcal{P}(U, ||.||, t) \le \left( 1 + \frac{2}{t} \right)^{n}
  \end{equation*}
\end{proposition}

\begin{theorem}[Covering argument]
  Let $B$ be an $N \times n$-matrix and let $\mathcal{N}$ be a $t$-covering of $S^{n - 1}$ for some $t \in [0, 1)$.
  \begin{enumerate}
  \item $\max_{x \in \mathcal{N}} ||Bx||_{2} \le ||B|| \le (1 - t)^{-1} \max_{x \in \mathcal{N}} ||Bx||_{2}$
  \item If furthermore $N = n$ and $B$ is symmetric
    \begin{equation*}
      \max_{x \in \mathcal{N}} |\langle Bx, x \rangle| \le ||B|| \le (1 - 2t)^{-1} \max_{x \in \mathcal{N}} \max_{x \in \mathcal{N}} |\langle Bx, x \rangle|
    \end{equation*}
  \end{enumerate}
\end{theorem}

\subsection{Bernstein's inequality}

\begin{theorem}[Bernstein's inequality]
  Let $X_{1}, \dots, X_{M}$ be independent centered subexponential random variables with parameters $(\beta, \kappa)$.
  Then for all $t > 0$ and an absolute constant $C$
  \begin{equation*}
    \Pr{\left| \sum_{k = 1}^{M} X_{l} \right| > t} \le 2\exp\left( -\frac{\kappa^{2} t^{2}}{4(2\beta M + \kappa t)} \right)
  \end{equation*}
\end{theorem}

\end{document}
