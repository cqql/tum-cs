\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% Define the page margin
\usepackage[margin=3cm]{geometry}

% Better typography (font rendering)
\usepackage{microtype}

% Math environments and macros
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

% Define \includegraphics to include graphics
\usepackage{graphicx}

% Draw graphics from a text description
\usepackage{tikz}

% Syntax highlighting
\usepackage{minted}

% Set global minted options
\setminted{linenos, autogobble, frame=lines, framesep=2mm}

% Import the comment environment for orgtbl-mode
\usepackage{comment}

% Do not indent paragraphs
\usepackage{parskip}

\title{Predicting Lost Packets}
\author{Marten Lienen, Julius Michaelis}

\begin{document}

\maketitle

\section{The Problem}

In network coding you group packets in generations of $n$ packets and send random linear combinations of them to your communication partners.
We know that the network is lossy -- the wireless network in particular -- and with this technique network coding already accounts for it because it does not matter which packets the destination receives as long as it receives $n$ packets at all, save for occasionally occuring combinations that are linearly dependent on previous packets.
So the sender has to keep sending random linear combinations until it receives $n$ ACKs.
This effort could be diminished if you knew in before how many packets would get lost and could inject a proportionate amount of redundancy into the network right from the beginning.

Our contributions to this problem are
\begin{itemize}
\item a model for estimation of the packet success probility distribution instead of only its mean
\item a formula to derive a number $n'$ of redundant packets to inject so that the destination receives $n$ packets with a certain probability
\end{itemize}

\section{Model}

A dataflow is a sequence of packets $p_{1}, p_{2}, \dots$ sent at times $t_{1}, t_{2}, \dots$ from a source $S$ to a destination $D$.
However the data transport medium is unreliable so that packets can go missing.
We track this with a sequence of random variables $P_{1}, P_{2}, \dots$ that are defined to be $1$ if $p_{i}$ was successfully delivered and $0$ otherwise.
This means that each of these sending procedures $P_{i}$ is a Poisson trial with success probability $s_{i}$.
The $s_{i}$ are then again random variables on the real interval $[0, 1]$.
We call them packet success probabilities because they describe the probability that a packet transfer is successful at time $t_{i}$.

A reasonable choice to model the probability distribution of the $s_{i}$ would be a Beta distribution with parameters $\alpha_{i} \ge 1$ and $\beta_{i} \ge 1$.
This distribution can model a uniform distribution on $[0, 1]$ as well as a smooth, unimodal distribution.

\section{Updating the model}

Whenever you register a packet loss, you add $1$ to the $\beta$ parameter of the current prior.
When a packet is received, you add $1$ to the $\alpha$ parameter of the current prior instead.

\section{Trust Function Model}

Say the last point of measurement was $t_{i}$ and we want to predict the distribution of the success probability $s'$ at time $t_{i} + t'$.
In the trust function model we still believe in the latest measurement, but we might believe less so depending on the time $t'$ that has passed.
This degree of trust is captured in a trust function $T(t) : \mathbb{R}_{\ge 0} \rightarrow [0, 1]$, where a value of $1$ means that you believe the old data completely while a value of $0$ means that you do not trust it at all.
Some reasonable things to expect from a trust function are
\begin{itemize}
\item $T(0) = 1$ because you should believe in your measurements
\end{itemize}

\subsection{$T = 0$}

\subsection{$T = 1$}

\subsection{$T(t) = \exp(-t)$}

\section{Predicting Redundancy}

Let $P \sim Beta(\alpha, \beta)$ be the current estimate for the packet success probability.
Then the amount of redundant packets necessary to send $n$ packets depending on $P$ is $X \mid P \sim NB(n, P)$ where $NB$ is the negative binomial distribution.
However, there is also a distribution for the joint distribution of $P$ and $X \mid P$ called the beta negative binomial distribution, so that
\begin{equation*}
  X \sim BNB(n, \alpha, \beta)
\end{equation*}
So $X$ is the number of redundant packets necessary to have $n$ successful transmissions when the packet success probability is beta distributed with parameters $\alpha$ and $\beta$ and therefore $X$ has support on $\mathbb{N}_{0}$.
In the end we are interested in the number of redundant packets $k$ that we have to send so that the destination receives $n$ packets with a probability of least some lower bound $p_{0}$.
More formally we are interested in a $k \in \mathbb{N}_{0}$ such that
\begin{equation*}
  P[X \le k] = CDF_{X}(k) \ge p_{0} \Leftrightarrow k = CDF_{X}^{-1}(p_{0})
\end{equation*}
Unfortunately there are neither closed-form expressions for the cumulative distribution function of a beta negative binomial distribution nor its inverse, so we have to resort to a simple summation over the PMF.
\begin{align*}
  CDF_{X}^{-1}(p_{0}) & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} P[X = i] \ge p_{0} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} \frac{\Gamma(n + i)}{i! \Gamma(n)} \frac{B(\alpha + n, \beta + i)}{B(\alpha, \beta)} \ge p_{0} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} \frac{\Gamma(n + i)}{i! \Gamma(n)} B(\alpha + n, \beta + i) \ge p_{0} \cdot B(\alpha, \beta) \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} \frac{\Gamma(n + i)}{i! \Gamma(n)} \frac{\Gamma(\alpha + n) \Gamma(\beta + i)}{\Gamma(\alpha + \beta + n + i)} \ge p_{0} \cdot \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} \frac{(n + i - 1)!}{i! (n - 1)!} \frac{\Gamma(\alpha + n) \Gamma(\beta + i)}{\Gamma(\alpha + \beta + n + i)} \ge p_{0} \cdot \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} \binom{n + i - 1}{i} \frac{\Gamma(\alpha + n) \Gamma(\beta + i)}{\Gamma(\alpha + \beta + n + i)} \ge p_{0} \cdot \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)} \right)\\
  \intertext{
  The problems with an actual implementation are twofold: First, the binomial coefficient and gamma function are computationally expensive and second, the evaluation of these terms produces intermediate results ranging from $10^{-50}$ to $10^{50}$ even for relatively small parameters.
  Keeping this in mind we will replace the gamma function with Stirling's approximation and evaluate the binomial coefficient with the recursive formula $\binom{n}{k} = \frac{n}{k} \cdot \binom{n - 1}{k - 1}$ to reuse results from previous iterations.
  Define $t_{0} = 1 = \binom{n - 1}{0}$ and $t_{i} = t_{i - 1} \cdot \frac{n + i - 1}{i} = \binom{n + i - 1}{i}$ for $i \in \mathbb{N}$.
  }
                      & \approx \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} t_{i} \frac{\frac{\sqrt{2\pi}}{\sqrt{\alpha + n}} \frac{(\alpha + n)^{\alpha + n}}{e^{\alpha + n}} \frac{\sqrt{2\pi}}{\sqrt{\beta + i}} \frac{(\beta + i)^{\beta + i}}{e^{\beta + i}}}{\frac{\sqrt{2\pi}}{\sqrt{\alpha + \beta + n + i}} \frac{(\alpha + \beta + n + i)^{\alpha + \beta + n + i}}{e^{\alpha + \beta + n + i}}} \ge p_{0} \cdot \frac{\frac{\sqrt{2\pi}}{\sqrt{\alpha}} \frac{\alpha^{\alpha}}{e^{\alpha}} \frac{\sqrt{2\pi}}{\sqrt{\beta}} \frac{\beta^{\beta}}{e^{\beta}}}{\frac{\sqrt{2\pi}}{\sqrt{\alpha + \beta}} \frac{(\alpha + \beta)^{\alpha + \beta}}{e^{\alpha + \beta}}} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \begin{array}{r} \sum_{i = 0}^{k} t_{i} \cdot \frac{\sqrt{2\pi} \sqrt{2\pi} \sqrt{\alpha + \beta + n + i} (\alpha + n)^{\alpha + n} (\beta + i)^{\beta + i} e^{\alpha + \beta + n + i}}{\sqrt{2\pi} \sqrt{\alpha + n} \sqrt{\beta + i} (\alpha + \beta + n + i)^{\alpha + \beta + n + i} e^{\alpha + n} e^{\beta + i}}\\ \ge p_{0} \cdot \frac{\sqrt{2\pi} \sqrt{2\pi} \sqrt{\alpha + \beta} \alpha^{\alpha} \beta^{\beta} e^{\alpha + \beta}}{\sqrt{2\pi} \sqrt{\alpha} \sqrt{\beta} (\alpha + \beta)^{\alpha + \beta} e^{\alpha} e^{\beta}} \end{array} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} t_{i} \cdot \frac{\sqrt{\alpha + \beta + n + i} (\alpha + n)^{\alpha + n} (\beta + i)^{\beta + i}}{\sqrt{\alpha + n} \sqrt{\beta + i} (\alpha + \beta + n + i)^{\alpha + \beta + n + i}} \ge p_{0} \cdot \frac{\sqrt{\alpha + \beta} \alpha^{\alpha} \beta^{\beta}}{\sqrt{\alpha} \sqrt{\beta} (\alpha + \beta)^{\alpha + \beta}} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \sum_{i = 0}^{k} t_{i} \cdot \sqrt{\frac{\alpha + n + \beta + i}{(\alpha + n)(\beta + i)}} \cdot \frac{(\alpha + n)^{\alpha + n} (\beta + i)^{\beta + i}}{(\alpha + \beta + n + i)^{\alpha + \beta + n + i}} \ge p_{0} \cdot \sqrt{\frac{\alpha + \beta}{\alpha\beta}} \cdot \frac{\alpha^{\alpha} \beta^{\beta}}{(\alpha + \beta)^{\alpha + \beta}} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \begin{array}{r} \sum_{i = 0}^{k} t_{i} \cdot \sqrt{\frac{\alpha + n + \beta + i}{(\alpha + n)(\beta + i)}} \cdot \left( \frac{\alpha + n}{\alpha + n + \beta + i} \right)^{\alpha + n} \cdot \left( \frac{\beta + i}{\alpha + n + \beta + i} \right)^{\beta + i}\\ \ge p_{0} \cdot \sqrt{\frac{\alpha + \beta}{\alpha\beta}} \cdot \left( \frac{\alpha}{\alpha + \beta} \right)^{\alpha} \cdot \left( \frac{\beta}{\alpha + \beta} \right)^{\beta} \end{array} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \begin{array}{r} \sum_{i = 0}^{k} t_{i} \cdot \sqrt{\frac{\alpha + n + \beta + i}{(\alpha + n)(\beta + i)}} \cdot \left( \frac{(\alpha + n)(\alpha + \beta)}{(\alpha + n + \beta + i)\alpha} \right)^{\alpha} \cdot \left( \frac{\alpha + n}{\alpha + n + \beta + i} \right)^{n} \cdot \left( \frac{(\beta + i)(\alpha + \beta)}{(\alpha + n + \beta + i)\beta} \right)^{\beta} \cdot \left( \frac{\beta + i}{\alpha + n + \beta + i} \right)^{i}\\ \ge p_{0} \cdot \sqrt{\frac{\alpha + \beta}{\alpha\beta}} \end{array} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \begin{array}{r} \sum_{i = 0}^{k} t_{i} \cdot \sqrt{\frac{\alpha + n + \beta + i}{\beta + i}} \cdot \left( \frac{(\alpha + n)(\alpha + \beta)}{(\alpha + n + \beta + i)\alpha} \right)^{\alpha} \cdot \left( \frac{\alpha + n}{\alpha + n + \beta + i} \right)^{n} \cdot \left( \frac{(\beta + i)(\alpha + \beta)}{(\alpha + n + \beta + i)\beta} \right)^{\beta} \cdot \left( \frac{\beta + i}{\alpha + n + \beta + i} \right)^{i}\\ \ge p_{0} \cdot \sqrt{\frac{(\alpha + \beta)(\alpha + n)}{\alpha\beta}} \end{array} \right)\\
                      & = \min_{k \in \mathbb{N}_{0}} \left( \begin{array}{r} \sum_{i = 0}^{k} t_{i} \cdot \left( \frac{(\alpha + n)(\alpha + \beta)}{(\alpha + n + \beta + i)\alpha} \right)^{\alpha} \cdot \left( \frac{\alpha + n}{\alpha + n + \beta + i} \right)^{n} \cdot \left( \frac{(\beta + i)(\alpha + \beta)}{(\alpha + n + \beta + i)\beta} \right)^{\beta} \cdot \left( \frac{\beta + i}{\alpha + n + \beta + i} \right)^{i - \frac{1}{2}}\\ \ge p_{0} \cdot \sqrt{\frac{(\alpha + \beta)(\alpha + n)}{\alpha\beta}} \end{array} \right)
\end{align*}

\section{Linux}

At least some of linux's wireless drivers use exponential moving average.

\begin{minted}{c}
  static void atmel_smooth_qual(struct atmel_private *priv)
  {
    unsigned long time_diff = (jiffies - priv->last_qual) / HZ;
    while (time_diff--) {
      priv->last_qual += HZ;
      priv->wstats.qual.qual = priv->wstats.qual.qual / 2;
      priv->wstats.qual.qual +=
        priv->beacons_this_sec * priv->beacon_period * (priv->wstats.qual.level + 100) / 4000;
      priv->beacons_this_sec = 0;
    }
    priv->wstats.qual.updated |= IW_QUAL_QUAL_UPDATED;
    priv->wstats.qual.updated &= ~IW_QUAL_QUAL_INVALID;
  }
\end{minted}

\end{document}
