\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% Define the page margin
\usepackage[margin=3cm]{geometry}

% Better typography (font rendering)
\usepackage{microtype}

% Math environments and macros
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

% Define \includegraphics to include graphics
\usepackage{graphicx}

% Do not indent paragraphs
\usepackage{parskip}

% Citations, bibliography
\usepackage{cite}

% Reformat enumeration labels
\usepackage{enumitem}

% Shallow fractions
\usepackage{nicefrac}

\newtheorem{theorem}{Theorem}

\DeclareMathOperator{\Tr}{Tr}

\title{Semidefinite Programming Approaches to Clustering}
\author{Marten Lienen}
\date{}

\begin{document}

\maketitle

\section{Introduction}

In this interdisciplinary project I have studied a recent paper on clustering, implemented the devised algorithm and did some theoretical work on a small detail that is required for an implementation.
The paper \cite{sdp} by Mixon, Villar and Ward presents an approximate k-medoid algorithm.

The setup is as follows: there are $k$ clusters.
For each cluster $t = 1, \dots, k$ we have $n_{t}$ datapoints $x_{t, l} \in \mathbb{R}^{m}, l = 1, \dots, n_{t}$, which were drawn according to a subgaussian probability distribution $\mathcal{D}_{t}$ with expectation $\gamma_{t}$ and covariance matrix $\Sigma_{t}$ with eigenvalues $0 < \sigma_{t, 1}^{2} \le \dots \le \sigma_{t, m}^{2}$.
Furthermore let $N = \sum_{t} n_{t}$ be the total number of datapoints and $x_{l}, l = 1, \dots, N$ be an enumeration of all datapoints $x_{t, l}$.

\section{The Semidefinite Programming Approach}

The k-medoid problem is then to select $k$ of the datapoints as approximations to the $k$ cluster centers $\gamma_{t}$.
However, their approach actually starts with the k-means clustering objective, i.e. solving the following minimization problem.
\begin{align*}
  \text{minimize} \quad & \sum_{t = 1}^{k} \sum_{i \in A_{t}} \left|\left| x_{i} - \frac{1}{|A_{t}|} \sum_{j \in A_{t}} x_{j} \right|\right|_{2}^{2}\\
  \text{subject to} \quad & \bigcup_{t} A_{t} = \{ 1, \dots, N \} \quad \text{and} \quad A_{s} \cap A_{t} = \emptyset~\forall s \ne t
\end{align*}
By defining the pairwise $N \times N$ squared-distance matrix $(D)_{ij} = ||x_{i} - x_{j}||_{2}^{2}$ and a cluster-affinity matrix $X$ given as
\begin{equation}
  X_{ij} = \begin{cases}
    \frac{1}{|A_{t}|} & \text{if $i$ and $j$ belong to the same cluster $t$}\\
    0 & \text{otherwise}
  \end{cases}
  \label{eq:x_r}
\end{equation}
you can rewrite the problem as
\begin{align*}
  \text{minimize} \quad & \Tr(DX)\\
  \text{subject to} \quad & \bigcup_{t} A_{t} = \{ 1, \dots, N \} \quad \text{and} \quad A_{s} \cap A_{t} = \emptyset~\forall s \ne t.
\end{align*}
This in turn has the following semidefinite relaxation.
\begin{align*}
  \text{minimize} \quad & \Tr(DX)\\
  \text{subject to} \quad & \Tr(X) = k\\
                        & X1 = 1\\
                        & X \ge 0\\
                        & X \succeq 0
\end{align*}

From this point on they proceed in three main steps.
First they define a matrix $R$ and show that the minimizer of the semidefinite program $X_{R}$ equals the definition in Equation \eqref{eq:x_r} if you replace $D$ by $R$.
Next they show that $D \approx R$ under the condition that the cluster centers are separated by at least $O(k\sigma_{max})$ and conclude through a series of steps that $X_{D} \approx X_{R}$ in the sense that $||X_{D} - X_{R}||_{F}^{2}$ is small.

The following steps are easily applicable to general distributions and datasets, though the paper's formal analysis is restricted to spherical Gaussians with variance $\sigma$ and the same number $n$ of datapoints drawn from each cluster.
Nonetheless the results from Section \ref{sec:results} demonstrate experimentally that their approach also works for differing numbers of datapoints and probability distributions.

In the second step called \emph{denoising} they define $P \in \mathbb{R}^{m \times N}$ as a matrix that has $x_{a, i}$ as its $(a, i)$-th column.
Then they argue that the matrix $PX_{R}$ has $\tilde{\gamma}_{a}$ as its $(a, i)$-th column and prove that the columns of $PX_{D}$ denoted by $c_{a, i}$ approximate the $\tilde{\gamma}_{a}$ as a consequence from the first step.
The $c_{a, i}$ are called the \emph{denoised datapoints}.
\emph{TODO: Add a figure for denoised data}

Lastly they perform the so called \emph{rounding}, an iterative algorithm that selects $k$ of the denoised datapoints as approximations to the $\tilde{\gamma}_{a}$s, i.e. the k-means-optimal centroids.
During the implementation of the rounding algorithm we noticed that it requires the choice of an $\varepsilon$ to construct a graph which in turn is based on the unknown $\Delta_{min}$.
Since this is the stimulus for our theory work in Section \ref{sec:epsilon}, we will present the central theorem and its proof in Section \ref{sec:rounding}.

\section{Selecting Cluster Center Approximations}
\label{sec:rounding}

\begin{theorem}
  Take $\varepsilon < \nicefrac{\Delta_{min}}{8}$, suppose
  \begin{equation*}
    \#\left\{ (a, i) : ||c_{a, i} - \tilde{\gamma}_{a}||_{2} > \varepsilon \right\} < \frac{n}{2}
  \end{equation*}
  and consider the graph $G$ of vertices $\{ c_{a, i} \}_{a = 1,}^{k}{}_{i = 1}^{n}$ such that $c_{a, i} \leftrightarrow c_{b, j}$ if $||c_{a, i} - c_{b, j}||_{2} \le 2\varepsilon$.
  For each $i = 1, \dots, k$ select the vertex $v_{i}$ of maximum degree (breaking ties arbitrarily) and update $G$ by removing every vertex $w$ such that $||w - v_{i}||_{2} \le 4\varepsilon$.
  Then there exists a permutation $\pi$ on $\{ 1, \dots, k \}$ such that
  \begin{equation*}
    ||v_{i} - \tilde{\gamma}_{\pi(i)}||_{2} \le 3\varepsilon
  \end{equation*}
  for every $i \in \{ 1, \dots, k \}$.
\end{theorem}

\begin{proof}
  We will prove the theorem by showing that a set of invariants holds across all $k$ iterations, one of which is the claim itself.
  Denote by $R_{i}$ the set of clusters for which we have not yet selected an approximation $v_{i}$, i.e. $R_{1} = \{ 1, \dots, k \}$ and $R_{i + 1} = R_{i} \setminus \{ \pi(i) \}$.
  Then the invariants read
  \begin{enumerate}[label=(\roman*)]
  \item \label{item:inv-outside} less than $\nicefrac{n}{2}$ vertices lie outside $\bigcup_{a \in R_{i}} B(\tilde{\gamma}_{a}, \varepsilon)$,
  \item \label{item:inv-inside} at least $\nicefrac{n}{2}$ vertices lie inside $B(v_{i}, 2\varepsilon)$ and
  \item \label{item:inv-unique} there exists a unique $a \in R_{i}$ such that $||v_{i} - \tilde{\gamma}_{a}||_{2} \le 3\varepsilon$
  \end{enumerate}
  where $B(x, r)$ is the closed ball around $x$ with radius $r$.
  First, we show that \ref{item:inv-outside} and \ref{item:inv-inside} imply \ref{item:inv-unique}.
  Afterwards we show by induction that \ref{item:inv-outside} and \ref{item:inv-inside} hold in every iteration and how to choose $\pi(i)$.

  From \ref{item:inv-outside} we know that there are at least $\nicefrac{n}{2}$ vertices in $B(v_{i}, 2\varepsilon)$.
  In combination with \ref{item:inv-inside} we get that at least one of them has to be in $B(\tilde{\gamma}_{a}, \varepsilon)$ for some $a \in R_{i}$ and thus $||v_{i} - \tilde{\gamma}_{a}||_{2} \le 3\varepsilon$ by the triangle inequality.
  Furthermore $3\varepsilon < \nicefrac{3}{8}\Delta_{min} < \nicefrac{\Delta_{min}}{2}$ making $a$ unique and we set $\pi(i) = a$ which gives us \ref{item:inv-unique}.

  In the first iteration \ref{item:inv-outside} is assumed and regarding \ref{item:inv-inside} we see that every ball $B(\tilde{\gamma}_{a}, \varepsilon)$ contains at least $\nicefrac{n}{2}$ vertices.
  Again using the triangle inequality we conclude that every vertex has a degree of at least $\nicefrac{n}{2} - 1$ and therefore at least $\nicefrac{n}{2}$ vertices in its $2\varepsilon$-ball.
  This is true in particular for the vertex of maximum degree, giving us \ref{item:inv-inside}.

  Assume \ref{item:inv-outside}, \ref{item:inv-inside} and \ref{item:inv-unique} for iteration $i < k$.
  Since $B(\tilde{\gamma}_{\pi(i)}, \varepsilon) \subset B(v_{i}, 4\varepsilon)$, the removal step removes all vertices inside $B(\tilde{\gamma}_{\pi(i)}, \varepsilon)$ and consequently ensures that \ref{item:inv-outside} holds for iteration $i + 1$.
  On top of this, if $v$ is a vertex to be removed, we have
  \begin{equation}
    ||v - \tilde{\gamma}_{\pi(i)}||_{2} \le ||v - v_{i}||_{2} + ||v_{i} - \tilde{\gamma}_{\pi(i)}||_{2} \le 7\varepsilon \label{eq:questionable}
  \end{equation}
  which implies that no vertices within an $\varepsilon$-radius of any other cluster center are removed.
  As a consequence invariant \ref{item:inv-inside} continues to hold in iteration $i + 1$ with the same argument as in the base case.
\end{proof}

This is how the proof is presented in the paper but on closer consideration the conclusion from Equation \eqref{eq:questionable} relies on the fact that $||\tilde{\gamma}_{a} - \tilde{\gamma}_{b}||_{2} > 8\varepsilon$ for all $a \ne b$.
Yet this might not be true because $8\varepsilon < \Delta_{min} \le ||\gamma_{a} - \gamma_{b}||_{2}$ but instead we are working with the random variable $||\tilde{\gamma}_{a} - \tilde{\gamma}_{b}||_{2}$ which could take on values less than $\Delta_{min}$.
We will touch on that topic again in Section \ref{sec:epsilon}.

Why is the assumption justified?
The problem with estimating $||\gamma_{a} - \gamma_{b}||$ through $||\tilde{\gamma}_{a} - \tilde{\gamma}_{b}||_{2}$.

\section{Choosing an $\varepsilon$}
\label{sec:epsilon}

Using theorem from \cite{tailbounds}.

\section{An Implementation}
\label{sec:results}

Slow, $O(n^{6})$, mostly used for verification.

\bibliography{report}{}
\bibliographystyle{plain}

\end{document}
