\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% Define the page margin
\usepackage[margin=3cm]{geometry}

% Better typography (font rendering)
\usepackage{microtype}

% Math environments and macros
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

% Define \includegraphics to include graphics
\usepackage{graphicx}

% Do not indent paragraphs
\usepackage{parskip}

% Citations, bibliography
\usepackage{cite}

% Reformat enumeration labels
\usepackage{enumitem}

% Shallow fractions
\usepackage{nicefrac}

% Put tildes under things
\def\utilde#1{\,\mathord{\vtop{\ialign{##\crcr
$\hfil\displaystyle{#1}\hfil$\crcr\noalign{\kern1.5pt\nointerlineskip}
$\hfil\widetilde{}\hfil$\crcr\noalign{\kern1.5pt}}}}\,}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\argmax}{arg\,max}

\title{Semidefinite Programming Approaches to Clustering}
\author{Marten Lienen}
\date{}

\begin{document}

\maketitle

\section{Introduction}

In this interdisciplinary project I have studied a recent paper on clustering, implemented the devised algorithm and did some theoretical work on a small detail that is required for an implementation.
The first pre-print \cite{sdp} of a paper by Mixon, Villar and Ward presents an approximate k-medoid algorithm.

The setup is as follows: there are $k$ clusters.
For each cluster $t = 1, \dots, k$ we have $n_{t}$ datapoints $x_{t, l} \in \mathbb{R}^{m}, l = 1, \dots, n_{t}$, which were drawn according to a subgaussian probability distribution $\mathcal{D}_{t}$ with expectation $\gamma_{t}$ and covariance matrix $\Sigma_{t}$ with eigenvalues $0 < \sigma_{t, 1}^{2} \le \dots \le \sigma_{t, m}^{2}$.
Furthermore let $N = \sum_{t} n_{t}$ be the total number of datapoints and $x_{l}, l = 1, \dots, N$ be an enumeration of all datapoints $x_{t, l}$.

Throughout their paper they often use the notation $(a, i)$ as an index which is to be read as a two-level index along a single dimension instead of two indices.
Finally we also need to introduce the k-means-optimal centroids $\tilde{\gamma}_{a} = \frac{1}{n_{a}} \sum_{i = 1}^{n_{a}} x_{a, i}$, i.e. the optimal estimators for $\gamma_{a}$ if you know the cluster affinities of each datapoint.

\emph{TODO: Relax and Round (SDP speak)}.

\section{The Semidefinite Programming Approach}

The k-medoid problem is then to select $k$ of the datapoints as approximations to the $k$ cluster centers $\gamma_{t}$.
However, their approach actually starts with the k-means clustering objective, i.e. solving the following minimization problem.
\begin{align*}
  \text{minimize} \quad & \sum_{t = 1}^{k} \sum_{i \in A_{t}} \left|\left| x_{i} - \frac{1}{|A_{t}|} \sum_{j \in A_{t}} x_{j} \right|\right|_{2}^{2}\\
  \text{subject to} \quad & \bigcup_{t} A_{t} = \{ 1, \dots, N \} \quad \text{and} \quad A_{s} \cap A_{t} = \emptyset~\forall s \ne t
\end{align*}
By defining the pairwise $N \times N$ squared-distance matrix $(D)_{ij} = ||x_{i} - x_{j}||_{2}^{2}$ and a cluster-affinity matrix $X$ given as
\begin{equation}
  X_{ij} = \begin{cases}
    \frac{1}{|A_{t}|} & \text{if $i$ and $j$ belong to the same cluster $t$}\\
    0 & \text{otherwise}
  \end{cases}
  \label{eq:x_r}
\end{equation}
you can rewrite the problem as
\begin{align*}
  \text{minimize} \quad & \Tr(DX)\\
  \text{subject to} \quad & \bigcup_{t} A_{t} = \{ 1, \dots, N \} \quad \text{and} \quad A_{s} \cap A_{t} = \emptyset~\forall s \ne t.
\end{align*}
This in turn has the following semidefinite relaxation.
\begin{align*}
  \text{minimize} \quad & \Tr(DX)\\
  \text{subject to} \quad & \Tr(X) = k\\
                        & X1 = 1\\
                        & X \ge 0\\
                        & X \succeq 0
\end{align*}

From this point on they proceed in three main steps.
First they define a matrix $R$ and show that the minimizer of the semidefinite program $X_{R}$ equals the definition in Equation \eqref{eq:x_r} if you replace $D$ by $R$.
Next they show that $D \approx R$ under the condition that the cluster centers are separated by at least $O(k\sigma_{max})$ and conclude through a series of steps that $X_{D} \approx X_{R}$ in the sense that $||X_{D} - X_{R}||_{F}^{2}$ is small.

The following steps are easily applicable to general distributions and datasets, though the paper's formal analysis is restricted to spherical Gaussians with variance $\sigma$ and the same number $n$ of datapoints drawn from each cluster.
Nonetheless the results from Section \ref{sec:results} demonstrate experimentally that their approach also works for differing numbers of datapoints and probability distributions.

In the second step called \emph{denoising} they define $P \in \mathbb{R}^{m \times N}$ as a matrix that has $x_{a, i}$ as its $(a, i)$-th column.
Then they argue that the matrix $PX_{R}$ has $\tilde{\gamma}_{a}$ as its $(a, i)$-th column and prove that the columns of $PX_{D}$ denoted by $c_{a, i}$ approximate the $\tilde{\gamma}_{a}$ as a consequence from the first step.
The $c_{a, i}$ are called the \emph{denoised datapoints}.
\emph{TODO: Add a figure for denoised data}

Lastly they perform the so called \emph{rounding}, an iterative algorithm that selects $k$ of the denoised datapoints as approximations to the $\tilde{\gamma}_{a}$s, i.e. the k-means-optimal centroids.
During the implementation of the rounding algorithm we noticed that it requires the choice of an $\varepsilon$ to construct a graph which in turn is based on the unknown $\Delta_{min}$.
Since this is the stimulus for our theory work in Section \ref{sec:epsilon}, we will present the central theorem and its proof in Section \ref{sec:rounding}.

\section{Selecting Cluster Center Approximations}
\label{sec:rounding}

\begin{theorem}
  \label{thm:rounding}
  Take $\varepsilon < \nicefrac{\Delta_{min}}{8}$, suppose
  \begin{equation*}
    \#\left\{ (a, i) : ||c_{a, i} - \tilde{\gamma}_{a}||_{2} > \varepsilon \right\} < \frac{n}{2}
  \end{equation*}
  and consider the graph $G$ of vertices $\{ c_{a, i} \}_{a = 1,}^{k}{}_{i = 1}^{n}$ such that $c_{a, i} \leftrightarrow c_{b, j}$ if $||c_{a, i} - c_{b, j}||_{2} \le 2\varepsilon$.
  For each $i = 1, \dots, k$ select the vertex $v_{i}$ of maximum degree (breaking ties arbitrarily) and update $G$ by removing every vertex $w$ such that $||w - v_{i}||_{2} \le 4\varepsilon$.
  Then there exists a permutation $\pi$ on $\{ 1, \dots, k \}$ such that
  \begin{equation*}
    ||v_{i} - \tilde{\gamma}_{\pi(i)}||_{2} \le 3\varepsilon
  \end{equation*}
  for every $i \in \{ 1, \dots, k \}$.
\end{theorem}

\begin{proof}
  We will prove the theorem by showing that a set of invariants holds across all $k$ iterations, one of which is the claim itself.
  Denote by $R_{i}$ the set of clusters for which we have not yet selected an approximation $v_{i}$, i.e. $R_{1} = \{ 1, \dots, k \}$ and $R_{i + 1} = R_{i} \setminus \{ \pi(i) \}$.
  Then the invariants read
  \begin{enumerate}[label=(\roman*)]
  \item \label{item:inv-outside} less than $\nicefrac{n}{2}$ vertices lie outside $\bigcup_{a \in R_{i}} B(\tilde{\gamma}_{a}, \varepsilon)$,
  \item \label{item:inv-inside} at least $\nicefrac{n}{2}$ vertices lie inside $B(v_{i}, 2\varepsilon)$ and
  \item \label{item:inv-unique} there exists a unique $a \in R_{i}$ such that $||v_{i} - \tilde{\gamma}_{a}||_{2} \le 3\varepsilon$
  \end{enumerate}
  where $B(x, r)$ is the closed ball around $x$ with radius $r$.
  First, we show that \ref{item:inv-outside} and \ref{item:inv-inside} imply \ref{item:inv-unique}.
  Afterwards we show by induction that \ref{item:inv-outside} and \ref{item:inv-inside} hold in every iteration and how to choose $\pi(i)$.

  From \ref{item:inv-outside} we know that there are at least $\nicefrac{n}{2}$ vertices in $B(v_{i}, 2\varepsilon)$.
  In combination with \ref{item:inv-inside} we get that at least one of them has to be in $B(\tilde{\gamma}_{a}, \varepsilon)$ for some $a \in R_{i}$ and thus $||v_{i} - \tilde{\gamma}_{a}||_{2} \le 3\varepsilon$ by the triangle inequality.
  Furthermore $3\varepsilon < \nicefrac{3}{8}\Delta_{min} < \nicefrac{\Delta_{min}}{2}$ making $a$ unique and we set $\pi(i) = a$ which gives us \ref{item:inv-unique}.

  In the first iteration \ref{item:inv-outside} is assumed and regarding \ref{item:inv-inside} we see that every ball $B(\tilde{\gamma}_{a}, \varepsilon)$ contains at least $\nicefrac{n}{2}$ vertices.
  Again using the triangle inequality we conclude that every vertex has a degree of at least $\nicefrac{n}{2} - 1$ and therefore at least $\nicefrac{n}{2}$ vertices in its $2\varepsilon$-ball.
  This is true in particular for the vertex of maximum degree, giving us \ref{item:inv-inside}.

  Assume \ref{item:inv-outside}, \ref{item:inv-inside} and \ref{item:inv-unique} for iteration $i < k$.
  Since $B(\tilde{\gamma}_{\pi(i)}, \varepsilon) \subset B(v_{i}, 4\varepsilon)$, the removal step removes all vertices inside $B(\tilde{\gamma}_{\pi(i)}, \varepsilon)$ and consequently ensures that \ref{item:inv-outside} holds for iteration $i + 1$.
  On top of this, if $v$ is a vertex to be removed, we have
  \begin{equation}
    ||v - \tilde{\gamma}_{\pi(i)}||_{2} \le ||v - v_{i}||_{2} + ||v_{i} - \tilde{\gamma}_{\pi(i)}||_{2} \le 7\varepsilon \label{eq:questionable}
  \end{equation}
  which implies that no vertices within an $\varepsilon$-radius of any other cluster center are removed.
  As a consequence invariant \ref{item:inv-inside} continues to hold in iteration $i + 1$ with the same argument as in the base case.
\end{proof}

This is how the proof is presented in the paper but on closer consideration the conclusion from Equation \eqref{eq:questionable} relies on the fact that $||\tilde{\gamma}_{a} - \tilde{\gamma}_{b}||_{2} > 8\varepsilon$ for all $a \ne b$.
Yet this might not be true because $8\varepsilon < \Delta_{min} \le ||\gamma_{a} - \gamma_{b}||_{2}$ but instead we are working with the random variable $||\tilde{\gamma}_{a} - \tilde{\gamma}_{b}||_{2}$ which could take on values less than $\Delta_{min}$.
We will touch on that topic again in Section \ref{sec:epsilon}.

The rounding algorithm has two preconditions: first, you have to choose an $\varepsilon < \nicefrac{\Delta_{min}}{8}$ and second, less than $\nicefrac{n}{2}$ denoised datapoints are allowed to be farther than an $\varepsilon$ away from their respective k-means-optimal centroid.
The former requires an estimate of $\Delta_{min}$ (see Sec. \ref{sec:epsilon}) while the latter can be argued from the following denoising result shown in the paper.

\vspace{.5em}
\begin{theorem}
  Suppose $\sigma \utilde{<} \Delta_{min} / \sqrt{k}$. Then
  \begin{equation}
    \frac{1}{N} \sum_{a = 1}^{k} \sum_{i = 1}^{n} ||c_{a, i} - \tilde{\gamma}_{a}||_{2}^{2} \utilde{<} \frac{||\Gamma||_{2 \rightarrow 2}^{2}}{\Delta_{min}^{2}} \cdot k\sigma^{2}
    \label{eq:denoising-bound}
  \end{equation}
  with high probability as $n \rightarrow \infty$.
  Here, the $a$th column of $\Gamma$ is $\tilde{\gamma}_{a} - \frac{1}{k} \sum_{b = 1}^{k} \tilde{\gamma}_{b}$.
\end{theorem}

This means that the mean squared error of the denoised datapoints is bounded by $k\sigma^{2}$ with high probability if none of the cluster center approximators is too far away from their mean, which is equivalent to the condition that no cluster is too far out insofar that the $\tilde{\gamma}_{a}$ converge to the true $\gamma_{a}$.
Taking the square root on both sides of Equation \eqref{eq:denoising-bound}, we get
\begin{equation*}
  \frac{1}{\sqrt{N}} \sqrt{\sum_{a = 1}^{k} \sum_{i = 1}^{n} ||c_{a, i} - \tilde{\gamma}_{a}||_{2}^{2}} \utilde{<} \sqrt{k} \sigma \utilde{<} \Delta_{min}.
\end{equation*}
A further application of Jensen's inequality for concave functions and dividing both sides by $\sqrt{N}$ yields
\begin{equation}
  \frac{1}{N} \sum_{a = 1}^{k} \sum_{i = 1}^{n} ||c_{a, i} - \tilde{\gamma}_{a}||_{2} \utilde{<} \frac{\Delta_{min}}{\sqrt{N}}.
  \label{eq:mean-error-bound}
\end{equation}
We see that the mean distance between $c_{a, i}$ and the associated $\tilde{\gamma}_{a}$ is bounded by $\nicefrac{\Delta_{min}}{\sqrt{N}}$.
Consequently there will be few denoised datapoints with $||c_{a, i} - \tilde{\gamma}_{a}||_{2} > \varepsilon$ for some $\varepsilon < \nicefrac{\Delta_{min}}{8}$ -- in particular less than $\nicefrac{n}{2}$ for $N$ large enough -- justifying the second precondition.
The following sketch shows that $N \in O(k^{2})$ datapoints suffice.
Let $p$ be the number of denoised datapoints with a distance $||c_{a, i} - \tilde{\gamma}_{a}||_{2}$ of at least $\varepsilon$.
Since all distances are non-negative, $p$ is maximized if the distances are minimized, i.e. $p$ points have a distance of $\varepsilon$ and the rest has a distance of $0$.
Assume that $\varepsilon$ is chosen not too small, i.e. $\nicefrac{\Delta_{min}}{\varepsilon} \le q$ for some $q > 8$.
\begin{equation*}
  \frac{1}{N} \cdot p \cdot \varepsilon \utilde{<} \frac{\Delta_{min}}{\sqrt{N}} \Leftrightarrow p \utilde{<} \frac{\Delta_{min}}{\varepsilon} \sqrt{N} < q \sqrt{N}
\end{equation*}
Now we can compute a lower bound for $N$ by checking when the upper bound is less than $\nicefrac{n}{2}$.
\begin{equation*}
  q \sqrt{N} = q \sqrt{kn} < \frac{n}{2} \Leftrightarrow (2q)^{2}k < n \Leftrightarrow N > (2q)^{2} k^{2} > 256k^{2}
\end{equation*}
This shows first that $N \in O(k^{2})$ datapoints suffice to satisfy the second condition of Theorem \ref{thm:rounding} and second that you want to choose an $\varepsilon$ as close to $\nicefrac{\Delta_{min}}{8}$ as possible to require fewer datapoints.

\section{Choosing an $\varepsilon$}
\label{sec:epsilon}

Using the following theorem from Tropp's paper \cite{tailbounds} on tail bounds for sums of random matrices.
\vspace{.5em}
\begin{theorem}[Matrix Bernstein]
  \label{thm:matrix-bernstein}
  Consider a finite sequence $\{ X_{k} \}$ of independent, random, self-adjoint matrices with dimension $d$.
  Assume that each random matrix satisfies
  \begin{equation*}
    \mathbb{E}\,X_{k} = 0 \quad \text{and} \quad \lambda_{max}(X_{k}) \le R\ \text{almost surely.}
  \end{equation*}
  Then, for all $t \ge 0$,
  \begin{equation*}
    \mathbb{P}\left( \lambda_{max}\left( \sum_{k} X_{k} \right) \ge t \right) \le d \cdot \exp\left( -\frac{\nicefrac{t^{2}}{2}}{\sigma^{2} + R \nicefrac{t}{3}} \right)
  \end{equation*}
  where $\sigma^{2} = ||\sum_{k} \mathbb{E}\,X_{k}^{2}||$.
\end{theorem}

Assume that $||x_{t, l} - \gamma_{t}||_{2} \le R_{t}$.
Otherwise truncation.

\vspace{.5em}
\begin{lemma}
  \begin{equation*}
    \mathbb{P}\left( ||\tilde{\gamma}_{t} - \gamma_{t}||_{2} \ge \delta \right) \le (m + 1) \cdot \exp\left( -\frac{\delta^{2} \nicefrac{n_{t}}{2}}{R_{t} \nicefrac{\delta}{3} + \sum_{l = 1}^{m} \sigma_{t, l}^{2}} \right)
  \end{equation*}
\end{lemma}

In the following we will use $x_{t, l}$ as random variables with distribution $\mathcal{D}_{t}$ instead of the concrete samples.
\begin{proof}
  Define random variables $y_{l} = x_{t, l} - \gamma_{t}$ and random, self-adjoint matrices
  \begin{equation*}
    Y_{l} = \begin{pmatrix}
      0 & y_{l}^{T}\\
      y_{l} & 0
    \end{pmatrix}.
  \end{equation*}
  Since the $y_{l}$ are independent and $\mathbb{E}\,y_{l} = 0$, the matrices $Y_{l}$ are independent with $\mathbb{E}\,Y_{l} = 0$ and $\lambda_{max}(Y_{l}) = ||y_{l}||_{2}$ (TODO: show this).
  Applying theorem \ref{thm:matrix-bernstein} gives us
  \begin{align*}
    \mathbb{P}(||\tilde{\gamma}_{t} - \gamma_{t}||_{2} \ge \delta) & = \mathbb{P}\left( \frac{1}{n_{t}} \left|\left| \sum_{l = 1}^{n_{t}} x_{t, l} - \gamma_{t} \right|\right|_{2} \ge \delta \right)\\
    & = \mathbb{P}\left( \left|\left| \sum_{l = 1}^{n_{t}} y_{l} \right|\right|_{2} \ge n_{t}\delta \right)\\
    & = \mathbb{P}\left( \lambda_{max}\left( \sum_{l = 1}^{n_{t}} Y_{l} \right) \ge n_{t}\delta \right)\\
    & \le (m + 1) \cdot \exp\left( -\frac{\delta^{2} \nicefrac{n_{t}^{2}}{2}}{R_{t} \nicefrac{\delta n_{t}}{3} + \sigma^{2}} \right)
  \end{align*}
  with $\sigma^{2} = \left|\left| \sum_{l} \mathbb{E}\, Y_{l}^{2} \right|\right|$.
  To determine the largest singular value of $\sigma^{2}$ we will first compute $\mathbb{E}\,Y_{l}^{2}$.
  \begin{equation*}
    \mathbb{E}\,Y_{l}^{2} = \mathbb{E}\,\begin{pmatrix}
      y_{l}^{T}y_{l} & 0\\
      0 & y_{l}y_{l}^{T}
    \end{pmatrix}
    = \begin{pmatrix}
      \Tr(\Sigma_{t}) & 0\\
      0 & \Sigma_{t}
    \end{pmatrix}
  \end{equation*}
  Now let $UWV^{*}$ be a singular value decomposition of $\Sigma_{t}$.
  Then we can write $\mathbb{E}\,Y_{l}$ as
  \begin{equation*}
    \mathbb{E}\,Y_{l}^{2} = \begin{pmatrix}
      1 & 0\\
      0 & U
    \end{pmatrix}
    \begin{pmatrix}
      \Tr(\Sigma_{t}) & 0\\
      0 & W
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0\\
      0 & V^{*}
    \end{pmatrix}.
  \end{equation*}
  Since the left- and right-hand matrix are unitary and the matrix in the middle is diagonal, this form is a singular value decomposition of $\mathbb{E}\,Y_{l}$ and the center matrix's diagonal carries the singular values.
  Therefore the maximum singular value of $\mathbb{E}\,Y_{l}^{2}$ is
  \begin{equation*}
    ||\mathbb{E}\,Y_{l}^{2}|| = \max\left( \Tr(\Sigma_{t}), \lambda_{max}(\Sigma_{t}) \right) = \max\left( \sum_{l = 1}^{m} \sigma_{t, l}^{2}, \sigma_{t, m}^{2} \right) = \sum_{l = 1}^{m} \sigma_{t, l}^{2}.
  \end{equation*}
  In combination with
  \begin{equation*}
    \sigma^{2} = \left|\left| \sum_{l = 1}^{n_{t}} \mathbb{E}\,Y_{l} \right|\right| = n_{t} \left|\left| \mathbb{E}\,Y_{1} \right|\right|
  \end{equation*}
  we get the claimed result.
\end{proof}

\vspace{.5em}
\begin{lemma}
  Let $a, b$ be the minimizers of $\min_{\substack{a, b\\a \ne b}} ||\gamma_{a} - \gamma_{b}||_{2}$.
  Then
  \begin{equation*}
    \mathbb{P}\left( \min_{\substack{s, t\\s \ne t}} ||\tilde{\gamma}_{s} - \tilde{\gamma}_{t}||_{2} - \Delta_{min} \ge \delta \right) \le (m + 1) \left( \exp\left( -\frac{\delta^{2} \nicefrac{n_{a}}{8}}{R_{a} \nicefrac{\delta}{6} + \sum_{l = 1}^{m} \sigma_{a, l}^{2}} \right) + \exp\left( -\frac{\delta^{2} \nicefrac{n_{b}}{8}}{R_{b} \nicefrac{\delta}{6} + \sum_{l = 1}^{m} \sigma_{b, l}^{2}} \right) \right).
  \end{equation*}
\end{lemma}

\begin{proof}

\end{proof}

\section{An Implementation}
\label{sec:results}

Slow, $O(n^{6})$, mostly used for verification.

Iterated application of denoising.

\bibliography{report}{}
\bibliographystyle{plain}

\end{document}
